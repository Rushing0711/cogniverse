# kvdç¬¬1ç«  Kubeadminå®‰è£…K8S V1.23

å•ç‚¹ç‰ˆæœ¬ï¼šhttps://blog.csdn.net/Josh_scott/article/details/121961369?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0.pc_relevant_default&spm=1001.2101.3001.4242.1&utm_relevant_index=3

é«˜å¯ç”¨ç‰ˆæœ¬ï¼šhttps://blog.csdn.net/qq_16538827/article/details/120175489

Kubeadmæ˜¯ä¸€ä¸ªK8séƒ¨ç½²å·¥å…·ï¼Œæä¾›kubeadm initå’Œkubeadm joinï¼Œç”¨äºå¿«é€Ÿéƒ¨ç½²Kubernetesé›†ç¾¤ã€‚

## 0 å…ˆå†³æ¡ä»¶

- ä¸€å°å…¼å®¹çš„Linuxä¸»æœºã€‚Kubernetesé¡¹ç›®ä¸ºåŸºäºDebianå’ŒRed Hatçš„Linuxå‘è¡Œç‰ˆä¸€çº§ä¸€äº›ä¸æä¾›åŒ…ç®¡ç†å™¨çš„å‘è¡Œç‰ˆæä¾›é€šç”¨çš„æŒ‡ä»¤ã€‚
- æ¯å°æœºå™¨ 2GB æˆ–æ›´å¤šçš„ RAMï¼ˆå¦‚æœå°‘äºè¿™ä¸ªæ•°å­—å°†ä¼šå½±å“ä½ åº”ç”¨çš„è¿è¡Œå†…å­˜ï¼‰ã€‚
- 2CPUæ ¸å¿ƒæˆ–æ›´å¤š
- é›†ç¾¤ä¸­çš„æ‰€æœ‰æœºå™¨çš„ç½‘ç»œå½¼æ­¤å‡èƒ½ç›¸äº’è¿æ¥ï¼ˆå…¬ç½‘å’Œå†…ç½‘éƒ½å¯ä»¥ï¼‰
  - **è®¾ç½®é˜²ç«å¢™æ”¾è¡Œè§„åˆ™**
- èŠ‚ç‚¹ä¹‹ä¸­ä¸å¯ä»¥æœ‰é‡å¤çš„ä¸»æœºåã€MACåœ°å€æˆ–product_uuidã€‚è¯·å‚è€ƒ[è¿™é‡Œ](https://v1-29.docs.kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/)äº†è§£æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
  - **è®¾ç½®ä¸åŒhostname**
- å¼€å¯æœºå™¨ä¸Šçš„æŸäº›ç«¯å£ã€‚è¯·å‚è€ƒ[è¿™é‡Œ](https://v1-29.docs.kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/)äº†è§£æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
  - **å†…ç½‘äº’ä¿¡**
- ç¦ç”¨äº¤æ¢åˆ†åŒºã€‚ä¸ºäº†ä¿è¯ kubelet æ­£å¸¸å·¥ä½œï¼Œä½ **å¿…é¡»**ç¦ç”¨äº¤æ¢åˆ†åŒºã€‚
  - **æ°¸ä¹…å…³é—­**
- <span style="color:red;font-weight:bold;font-size:20px;">è‹¥æ— ç‰¹æ®Šè¯´æ˜ï¼Œä¸‹é¢éƒ½æ˜¯ä½¿ç”¨rootç”¨æˆ·æ‰§è¡Œå‘½ä»¤</span>

## 1 åŸºç¡€ç¯å¢ƒå‡†å¤‡

### 1.1 æœåŠ¡å™¨è§„åˆ’

| æœºå™¨å | ç³»ç»Ÿç±»å‹ | IPåœ°å€          | CPU  | å†…å­˜ | éƒ¨ç½²å†…å®¹ |
| ------ | -------- | --------------- | ---- | ---- | -------- |
| emon   | Rocky9.5 | 192.168.200.116 | 2æ ¸  | >=2G | master   |
| emon2  | Rocky9.5 | 192.168.200.117 | 2æ ¸  | >=2G | worker   |
| emon3  | Rocky9.5 | 192.168.200.118 | 2æ ¸  | >=2G | worker   |

### 1.2 ç³»ç»Ÿå®‰è£…ï¼ˆæ‰€æœ‰èŠ‚ç‚¹ï¼‰

[ç³»ç»Ÿå®‰è£…](http://localhost:8751/devops/new/Linux/01-%E7%AC%AC1%E7%AB%A0%20%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85.html)

### 1.3 ç³»ç»Ÿè®¾ç½®ï¼ˆæ‰€æœ‰èŠ‚ç‚¹ï¼‰

#### 1.3.1 ä¸»æœºå

ä¸»æœºåå¿…é¡»æ¯ä¸ªèŠ‚ç‚¹éƒ½ä¸ä¸€æ ·ï¼ˆå»ºè®®å‘½åè§„èŒƒï¼šæ•°å­—+å­—æ¯+ä¸­åˆ’çº¿ç»„åˆï¼Œä¸è¦åŒ…å«å…¶ä»–ç‰¹æ®Šå­—ç¬¦ï¼‰ã€‚

```bash
# æŸ¥çœ‹ä¸»æœºå
$ hostname
# è®¾ç½®ä¸»æœºåï¼šæ³¨æ„ä¿®æ”¹ä¸ºå…·ä½“çš„ä¸»æœºå
$ hostnamectl set-hostname emon
```

#### 1.3.2 æœ¬åœ°DNS

é…ç½®hostï¼Œä½¿å¾—æ‰€æœ‰èŠ‚ç‚¹ä¹‹é—´å¯ä»¥é€šè¿‡hostnameäº’ç›¸è®¿é—®ã€‚

```bash
$ sudo cat <<-'EOF' | sudo tee -a /etc/hosts
192.168.200.116	emon
192.168.200.117 emon2
192.168.200.118 emon3
EOF
```

#### 1.3.3 å®‰è£…ä¾èµ–åŒ…

```bash
# æ›´æ–°yum
$ dnf update -y
# å®‰è£…ä¾èµ–åŒ…
$ dnf install -y socat conntrack ipvsadm ipset jq sysstat curl iptables libseccomp yum-utils
```

#### 1.3.4 å…³é—­é˜²ç«å¢™ã€é‡ç½®iptablesã€å…³é—­swapã€å…³é—­selinuxå’Œdnsmasq

```bash
# å…³é—­é˜²ç«å¢™
$ systemctl stop firewalld && systemctl disable firewalld

# è®¾ç½®iptablesè§„åˆ™
$ iptables -F && iptables -X && iptables -F -t nat && iptables -X -t nat && iptables -P FORWARD ACCEPT

# å…³é—­swap
$ swapoff -a
# å»æ‰swapå¼€æœºå¯åŠ¨
$ sed -i '/swap/s/^\(.*\)$/#\1/g' /etc/fstab

# å…³é—­selinux
$ setenforce 0
# é˜²æ­¢é‡å¯æ¢å¤
$ sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config

# å…³é—­dnsmasqï¼ˆå¦åˆ™å¯èƒ½å¯¼è‡´dockerå®¹å™¨æ— æ³•è§£æåŸŸåï¼‰ï¼šå¦‚æœæ²¡æœ‰è¯¥å¯åŠ¨å•å…ƒï¼Œå¯ä»¥å¿½ç•¥ï¼
$ systemctl stop dnsmasq && systemctl disable dnsmasq
```

#### 1.3.5 ç³»ç»Ÿå‚æ•°è®¾ç½®

```bash
# å°†æ¡¥æ¥çš„IPv4æµé‡ä¼ é€’åˆ° iptables çš„é“¾ï¼š
$ cat > /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_nonlocal_bind = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0
vm.overcommit_memory = 1
EOF

# ç”Ÿæ•ˆæ–‡ä»¶
$ sysctl -p /etc/sysctl.d/kubernetes.conf
```

> å¦‚æœæ‰§è¡Œsysctl -pæŠ¥é”™ï¼š
>
> > sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-ip6tables: æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•
> >
> > sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•
>
> å¯èƒ½çš„åŸå› åˆ†æï¼š
>
> 1. **å†…æ ¸æ¨¡å—æœªåŠ è½½**ï¼š
>    - `bridge` å’Œ `br_netfilter` å†…æ ¸æ¨¡å—æœªæ¿€æ´»
>    - è¿™äº›æ¨¡å—æä¾›ç½‘ç»œæ¡¥æ¥å’Œé˜²ç«å¢™è¿‡æ»¤åŠŸèƒ½
> 2. **ç³»ç»Ÿé…ç½®é—®é¢˜**ï¼š
>    - å†…æ ¸å‚æ•°æœªæ­£ç¡®è®¾ç½®
>    - å¸¸è§äºæ–°å®‰è£…çš„ç³»ç»Ÿæˆ–äº‘æœåŠ¡å™¨
> 3. **å®¹å™¨å¹³å°ä¾èµ–**ï¼š
>    - Docker/Kubernetes éœ€è¦è¿™äº›æ¨¡å—å®ç°å®¹å™¨ç½‘ç»œ
>
> - æ­¥éª¤ä¸€ï¼šç¡®ä¿æ¨¡å—æŒä¹…åŒ–ï¼ˆé‡å¯åæœ‰æ•ˆï¼‰
>
> ```bash
> # åˆ›å»ºæ¨¡å—åŠ è½½é…ç½®æ–‡ä»¶
> $ cat <<EOF | sudo tee /etc/modules-load.d/br_netfilter.conf
> bridge
> br_netfilter
> EOF
> 
> # éªŒè¯é…ç½®
> $ sudo systemctl restart systemd-modules-load.service
> ```
>
> - æ­¥éª¤äºŒï¼šé…ç½®åï¼Œå†é‡æ–°æ‰§è¡Œâ€œç”Ÿæ•ˆæ–‡ä»¶â€çš„å‘½ä»¤
>
> ```bash
> # ç”Ÿæ•ˆæ–‡ä»¶
> $ sysctl -p /etc/sysctl.d/kubernetes.conf
> ```
>
> - æ­¥éª¤ä¸‰ï¼šéªŒè¯
>
> ```bash
> # æ£€æŸ¥å‚æ•°æ˜¯å¦ç”Ÿæ•ˆ
> sysctl net.bridge.bridge-nf-call-iptables
> sysctl net.bridge.bridge-nf-call-ip6tables
> 
> # åº”è¿”å›ï¼š
> # net.bridge.bridge-nf-call-iptables = 1
> # net.bridge.bridge-nf-call-ip6tables = 1
> ```

#### 1.3.6 é…ç½®SSHå…å¯†ç™»å½•ï¼ˆä»…ä¸­è½¬èŠ‚ç‚¹ï¼‰

ä¸ºäº†æ–¹ä¾¿æ–‡ä»¶çš„copyæˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªä¸­è½¬èŠ‚ç‚¹ï¼ˆéšä¾¿ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå¯ä»¥æ˜¯é›†ç¾¤ä¸­çš„ä¹Ÿå¯ä»¥æ˜¯éé›†ç¾¤ä¸­çš„ï¼‰ï¼Œé…ç½®å¥½è·Ÿå…¶ä»–æ‰€æœ‰èŠ‚ç‚¹çš„å…å¯†ç™»å½•ã€‚è¿™é‡Œé€‰æ‹©emonèŠ‚ç‚¹ï¼š

```bash
# çœ‹çœ‹æ˜¯å¦å·²ç»å­˜åœ¨rsaå…¬é’¥
$ cat ~/.ssh/id_rsa.pub

# å¦‚æœä¸å­˜åœ¨å°±åˆ›å»ºä¸€ä¸ªæ–°çš„
$ ssh-keygen -t rsa

# æŠŠid_rsa.pubæ–‡ä»¶å†…å®¹copyåˆ°å…¶ä»–æœºå™¨çš„æˆæƒæ–‡ä»¶ä¸­
$ ssh-copy-id -i ~/.ssh/id_rsa.pub emon
$ ssh-copy-id -i ~/.ssh/id_rsa.pub emon2
$ ssh-copy-id -i ~/.ssh/id_rsa.pub emon3
```

#### 1.3.7 ç§»é™¤dockerç›¸å…³è½¯ä»¶åŒ…ï¼ˆå¯é€‰ï¼‰

```bash
$ dnf remove -y docker* container-selinux
$ rm -f /etc/docker/daemon.json
$ rm -rf /var/lib/docker/
```

å¦‚æœyumæŠ¥å‘Šè¯´ä»¥ä¸Šå®‰è£…åŒ…æœªå®‰è£…ï¼ŒæœªåŒ¹é…ï¼Œæœªåˆ é™¤ä»»ä½•å®‰è£…åŒ…ï¼Œè¡¨ç¤ºç¯å¢ƒå¹²å‡€ï¼Œæ²¡æœ‰å†å²é—ç•™æ—§ç‰ˆå®‰è£…ã€‚



## 2 å®‰è£…åŸºç¡€å·¥å…·ï¼ˆæ‰€æœ‰èŠ‚ç‚¹ï¼‰

### 2.1 å®‰è£…Docker

å‚è€ƒï¼š[Dockerçš„å®‰è£…ä¸é…ç½®.md](http://localhost:8751/devops/new/Docker/01-%E7%AC%AC1%E7%AB%A0%20Docker%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.html)


### 2.2 å®‰è£…kubeadm/kubelet/kubectl

K8Sä¾èµ–çš„Dockeræœ€ä½³ç‰ˆæœ¬ï¼š 20.10

https://github.com/kubernetes/kubernetes/blob/release-1.23/build/dependencies.yaml

1. è®¾ç½®k8sæº

å‚è€ƒï¼šhttps://mirrors.aliyun.com/kubernetes/yum/repos/

:::code-group

```bash [CentOS7.5]
$ cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes] 
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 
enabled=1 
gpgcheck=0 
repo_gpgcheck=0 
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg 
EOF

# å¯ä»¥æ›´æ–°/ç¼“å­˜ï¼Œä¹Ÿå¯ä»¥å¿½ç•¥
$ yum clean all && yum makecache
```

```bash [Rocky9.5]
# æ­¤æ“ä½œä¼šè¦†ç›– /etc/yum.repos.d/kubernetes.repo ä¸­ç°å­˜çš„æ‰€æœ‰é…ç½®
$ cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes] 
name=Kubernetes ARM
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-aarch64 
enabled=1 
gpgcheck=0 
repo_gpgcheck=0 
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg 
EOF

# å¯ä»¥æ›´æ–°/ç¼“å­˜ï¼Œä¹Ÿå¯ä»¥å¿½ç•¥
$ dnf clean all && dnf makecache
```

:::

2. å¯ä»¥æŸ¥çœ‹æ‰€æœ‰ä»“åº“ä¸­æ‰€æœ‰k8sç‰ˆæœ¬ï¼Œå¹¶é€‰æ‹©å®‰è£…ç‰¹å®šçš„ç‰ˆæœ¬

```bash
$ dnf list kubelet --showduplicates |sort -r
```

3. å®‰è£…kubeadm/kubelet/kubectl

```bash
# æŸäº› Linux å‘è¡Œç‰ˆï¼ˆå¦‚ RHEL/CentOS/Rockyï¼‰çš„ YUM/DNF é…ç½®ä¸­é»˜è®¤æ’é™¤äº† Kubernetes ç›¸å…³åŒ…ï¼Œè¿™é‡Œä¸´æ—¶ç¦ç”¨é’ˆå¯¹ Kubernetes çš„æ’é™¤è§„åˆ™ï¼Œå…è®¸å®‰è£… kube* å¼€å¤´çš„åŒ…
$ dnf install -y kubelet-1.23.17 kubeadm-1.23.17 kubectl-1.23.17 --disableexcludes=kubernetes
# kubeadm init å’Œ kubeadm join éƒ½ä¼šè§¦å‘å¯åŠ¨ kubeletï¼Œä½†ä¼šç»™è­¦å‘Šï¼›ä½†è‹¥ç›´æ¥å¯åŠ¨ kubelet åˆ™æ¯éš”å‡ ç§’å°±ä¼šé‡å¯ï¼Œå› ä¸ºå®ƒé™·å…¥äº†ä¸€ä¸ªç­‰å¾… kubeadm æŒ‡ä»¤çš„æ­»å¾ªç¯ã€‚ï¼ˆmasterå’ŒworkerèŠ‚ç‚¹ï¼‰ï¼Œè¿™é‡Œé€‰æ‹©ä»…åŠ å…¥å¼€æœºå¯åŠ¨ï¼Œå¹¶ä¸ç›´æ¥å¯åŠ¨ã€‚
$ systemctl enable kubelet
```

## 3 kubeadmåˆ›å»ºé›†ç¾¤ï¼ˆä»…masterèŠ‚ç‚¹ï¼‰

### 3.0 é¢„ä¸‹è½½é•œåƒï¼ˆå¼€å¯Dockerä»£ç†å¯å¿½ç•¥ï¼‰

- æŸ¥çœ‹ä¾èµ–é•œåƒ

```bash
$ kubeadm config images list
```

- é…ç½®å¹¶æ‰§è¡Œè„šæœ¬

```bash
$ vim master_images.sh
```

```bash
#!/bin/bash

images=(
	kube-apiserver:v1.23.17
	kube-controller-manager:v1.23.17
	kube-scheduler:v1.23.17
	kube-proxy:v1.23.17
	pause:3.6
	etcd:3.5.6-0
	coredns/coredns:v1.8.6
)

for imageName in ${images[@]} ; do
    docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
#   docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName  k8s.gcr.io/$imageName
done
# è‹¥ä¸å¸Œæœ›åˆ¶å®škubeadm initçš„é•œåƒ--image-repositoryï¼Œè¿™é‡Œå¯ä»¥æ”¾å¼€docker tagåˆ°k8s.gcr.io
```

```bash
$ chmod +x master_images.sh
```

- æ‰§è¡Œ

```bash
$ sh master_images.sh
```

### 3.1 kubeadm init

- åˆå§‹åŒ–

```bash
# åœ¨Masterä¸Šæ‰§è¡Œï¼Œç”±äºé»˜è®¤æ‹‰å–é•œåƒåœ°å€ k8s.gcr.io å›½å†…æ— æ³•è®¿é—®ï¼Œè¿™é‡ŒæŒ‡å®šé˜¿é‡Œäº‘é•œåƒä»“åº“åœ°å€ã€‚
# æ‰§è¡Œè¯¥æ­¥éª¤ä¹‹å‰ï¼Œä¹Ÿå¯ä»¥æ‰§è¡Œ kubeadm config images pull é¢„ä¸‹è½½é•œåƒ
# æŸ¥çœ‹é•œåƒ kubeadm config images list æŸ¥çœ‹é»˜è®¤é…ç½® kubeadm config print init-defaults
# Classless Inter-Domain Routing (CIDR)ï¼Œä¸­æ–‡è¯‘ä¸ºæ— ç±»åˆ«åŸŸé—´è·¯ç”±ï¼Œæ˜¯äº’è”ç½‘ä¸­ç”¨äºæ›´æœ‰æ•ˆåˆ†é…å’Œè·¯ç”± IP åœ°å€ï¼ˆä¸»è¦æ˜¯ IPv4ï¼‰çš„ä¸€ç§æ–¹æ³•ã€‚
# --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers æŒ‡å®šé•œåƒåœ°å€ï¼Œé»˜è®¤æ˜¯ k8s.gcr.io
# é•œåƒåœ°å€ä¹Ÿå¯ä»¥æ˜¯ registry.aliyuncs.com/google_containers è¯·æ³¨æ„ï¼šæ²¡æœ‰å¼€å¯Dockerä»£ç†æœåŠ¡å™¨æ—¶å¿…é¡»æŒ‡å®š
# --control-plane-endpoint æ˜¯éƒ¨ç½² Kubernetes HA æ§åˆ¶å¹³é¢çš„å…³é”®æ­¥éª¤ï¼Œå•masterèŠ‚ç‚¹æ—¶å¯ä»¥ä¸é…ç½®ï¼Œå¤šmasterèŠ‚ç‚¹æ—¶å¿…é¡»æŒ‡å®šï¼Œè€Œä¸”è¦æœ‰è´Ÿè½½å‡è¡¡å™¨æ¥è§£æè¯¥é…ç½®é¡¹åˆ°å…·ä½“masterèŠ‚ç‚¹ã€‚
$ kubeadm init \
--apiserver-advertise-address=192.168.200.116 \
--control-plane-endpoint=emon \
--kubernetes-version v1.23.17 \
--service-cidr=10.96.0.0/16 \
--pod-network-cidr=10.244.0.0/16

# ä½¿ç”¨ kubectl å·¥å…·ï¼ˆMaster&&NodeèŠ‚ç‚¹ï¼‰
$ mkdir -p $HOME/.kube 
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config 
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

# ã€äºŒé€‰ä¸€ã€‘å¦‚æœæ˜¯rootç”¨æˆ·ï¼Œå¯ä»¥ä½¿ç”¨å¦‚ä¸‹é…ç½®æ›¿æ¢ä¸Šé¢ï¼šï¼ˆä¸ä¸Šé¢äºŒé€‰ä¸€ï¼‰
export KUBECONFIG=/etc/kubernetes/admin.conf

# ã€ä¸´æ—¶ã€‘æ— éœ€æ‰§è¡Œï¼Œä»…åšè®°å½•å‚è€ƒ
# ==============================åˆå§‹åŒ–éƒ¨åˆ†æ—¥å¿—==============================
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join emon:6443 --token ldsakh.zkzpetkutui6ypmp \
        --discovery-token-ca-cert-hash sha256:7268baf811b3f1f2ca1e657fe90db99b8d3ed3f9efb8be03811b809d8efa5c5e \
        --control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join emon:6443 --token ldsakh.zkzpetkutui6ypmp \
        --discovery-token-ca-cert-hash sha256:7268baf811b3f1f2ca1e657fe90db99b8d3ed3f9efb8be03811b809d8efa5c5e
```

> å¸¦æœ‰`â€“control-plane`çš„å‘½ä»¤ï¼Œæ˜¯æ·»åŠ ä¸€ä¸ªä¸»èŠ‚ç‚¹ï¼›
>
> ä¸å¸¦æœ‰`â€“control-plane`çš„å‘½ä»¤ï¼Œæ˜¯æ·»åŠ ä¸€ä¸ªå·¥ä½œç‚¹ï¼›

- ç­‰å¾…ä¸€å°ä¼šåï¼ŒæŸ¥çœ‹å½“å‰pods

```bash
$ kubectl get pods -n kube-system
NAME                           READY   STATUS    RESTARTS   AGE
coredns-bd6b6df9f-llqtt        0/1     Pending   0          10m
coredns-bd6b6df9f-lsj6h        0/1     Pending   0          10m
etcd-emon                      1/1     Running   0          10m
kube-apiserver-emon            1/1     Running   0          10m
kube-controller-manager-emon   1/1     Running   0          10m
kube-proxy-r2jpz               1/1     Running   0          10m
kube-scheduler-emon            1/1     Running   0          10m

$ kubectl get pods -n kube-system -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP                NODE     NOMINATED NODE   READINESS GATES
coredns-bd6b6df9f-llqtt        0/1     Pending   0          11m   <none>            <none>   <none>           <none>
coredns-bd6b6df9f-lsj6h        0/1     Pending   0          11m   <none>            <none>   <none>           <none>
etcd-emon                      1/1     Running   0          11m   192.168.200.116   emon     <none>           <none>
kube-apiserver-emon            1/1     Running   0          11m   192.168.200.116   emon     <none>           <none>
kube-controller-manager-emon   1/1     Running   0          11m   192.168.200.116   emon     <none>           <none>
kube-proxy-r2jpz               1/1     Running   0          11m   192.168.200.116   emon     <none>           <none>
kube-scheduler-emon            1/1     Running   0          11m   192.168.200.116   emon     <none>           <none>

$ kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   12m

$ kubectl get nodes
NAME   STATUS     ROLES                  AGE   VERSION
emon   NotReady   control-plane,master   13m   v1.23.17
```

åˆ†æï¼šcorednsæ˜¯PendingçŠ¶æ€ï¼Œè¡¨ç¤ºç¼ºå°‘ç½‘ç»œæ’ä»¶ï¼Œä¸‹é¢å¼€å§‹å®‰è£…ç½‘ç»œæ’ä»¶ï¼

ç½‘ç»œæ’ä»¶åˆ—è¡¨ï¼š https://kubernetes.io/zh-cn/docs/concepts/cluster-administration/addons/

### 3.2 ç½‘ç»œæ’ä»¶å¤šé€‰1-[Calico](https://www.tigera.io/project-calico/)ï¼ˆä»…masterèŠ‚ç‚¹ï¼‰

GitHubï¼š https://github.com/projectcalico/calico

å®˜ç½‘ï¼šhttps://docs.tigera.io/archive

ç³»ç»Ÿéœ€æ±‚ï¼š https://docs.tigera.io/calico/latest/getting-started/kubernetes/requirements

#### 3.2.1 åˆ‡æ¢ç›®å½•

```bash
$ cd
$ mkdir -pv /root/k8s_soft/k8s_v1.23.17 && cd /root/k8s_soft/k8s_v1.23.17
```

è¿™éƒ¨åˆ†æˆ‘ä»¬éƒ¨ç½²kubernetesçš„ç½‘ç»œæŸ¥ä»¶ CNIã€‚

æ–‡æ¡£åœ°å€ï¼šhttps://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises

#### 3.2.2 ä¸‹è½½æ–‡ä»¶ä¸é…ç½®è°ƒæ•´

æ–‡æ¡£ä¸­æœ‰ä¸¤ä¸ªé…ç½®ï¼Œ50ä»¥ä¸‹èŠ‚ç‚¹å’Œ50ä»¥ä¸ŠèŠ‚ç‚¹ï¼Œå®ƒä»¬çš„ä¸»è¦åŒºåˆ«åœ¨äºè¿™ä¸ªï¼štyphaã€‚
å½“èŠ‚ç‚¹æ•°æ¯”è¾ƒå¤šçš„æƒ…å†µä¸‹ï¼ŒCalico çš„ Felixç»„ä»¶å¯é€šè¿‡ Typha ç›´æ¥å’Œ Etcd è¿›è¡Œæ•°æ®äº¤äº’ï¼Œä¸é€šè¿‡ kube-apiserverï¼Œé™ä½kube-apiserverçš„å‹åŠ›ã€‚å¤§å®¶æ ¹æ®è‡ªå·±çš„å®é™…æƒ…å†µé€‰æ‹©ä¸‹è½½ã€‚
ä¸‹è½½åçš„æ–‡ä»¶æ˜¯ä¸€ä¸ªall-in-oneçš„yamlæ–‡ä»¶ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨æ­¤åŸºç¡€ä¸Šåšå°‘è®¸ä¿®æ”¹å³å¯ã€‚

```bash
# ä¸‹è½½calico.yamlæ–‡ä»¶
# $ curl https://projectcalico.docs.tigera.io/manifests/calico.yaml -O ä¼šåŠ è½½æœ€æ–°ç‰ˆæœ¬ï¼Œå¯¹K8Sç‰ˆæœ¬V1.23.17ä¸å†é€‚åˆã€‚
# å…¼å®¹k8s v1.23.17ç‰ˆæœ¬ï¼Œæ”¯æŒå¤šæ¶æ„çš„ç½‘ç»œæ’ä»¶ç‰ˆæœ¬æ˜¯ v3.24.5ï¼Œå¯ä»¥å¦‚ä¸‹æ‰§è¡Œï¼›ä½†è‹¥éœ€è¦ä¿®æ”¹ä¸€äº›é…ç½®ï¼Œå¯ä»¥å…ˆä¸‹è½½
# kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.24.5/manifests/calico.yaml
$ curl https://docs.tigera.io/archive/v3.24/manifests/calico.yaml -O
```

ä¿®æ”¹IPè‡ªåŠ¨å‘ç°ï¼š

> å½“kubeletçš„å¯åŠ¨å‚æ•°ä¸­å­˜åœ¨--node-ipçš„æ—¶å€™ï¼Œä»¥host-networkæ¨¡å¼å¯åŠ¨çš„podçš„status.hostIPå­—æ®µå°±ä¼šè‡ªåŠ¨å¡«å…¥kubeletä¸­æŒ‡å®šçš„ipåœ°å€ã€‚

```js
- name: IP
  value: "autodetect" // [!code --]
  valueFrom: // [!code ++]
    fieldRef: // [!code ++]
      fieldPath: status.hostIP // [!code ++]
```

ä¿®æ”¹CIDRï¼šä¿®æ”¹æˆä½ è‡ªå·±çš„pod-network-cidrç½‘æ®µçš„valueï¼Œæˆ‘è¿™é‡Œæ˜¯10.244.0.0/16

```js
# - name: CALICO_IPV4POOL_CIDR // [!code --]
#   value: "192.168.0.0/16" // [!code --]
- name: CALICO_IPV4POOL_CIDR // [!code ++]
  value: "10.244.0.0/16" // [!code ++]
```

#### 3.2.3 æ‰§è¡Œå®‰è£…

```bash
# ç”Ÿæ•ˆä¹‹å‰æŸ¥çœ‹
$ kubectl get nodes
NAME   STATUS     ROLES                  AGE     VERSION
emon   NotReady   control-plane,master   5m31s   v1.23.17
# ä½¿ä¹‹ç”Ÿæ•ˆ
$ kubectl apply -f calico.yaml
# æŸ¥çœ‹pod
$ kubectl get po -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-577f77cb5c-g78c7   1/1     Running   0          13h
calico-node-hxvx8                          1/1     Running   0          13h
coredns-7f89b7bc75-8ks6f                   1/1     Running   0          14h
coredns-7f89b7bc75-kfdbm                   1/1     Running   0          14h
etcd-emon                                  1/1     Running   0          14h
kube-apiserver-emon                        1/1     Running   0          14h
kube-controller-manager-emon               1/1     Running   0          14h
kube-proxy-f2r8l                           1/1     Running   0          14h
kube-scheduler-emon                        1/1     Running   0          14h
# æŸ¥çœ‹node
$ kubectl get nodes
NAME    STATUS     ROLES                  AGE    VERSION
emon    Ready      control-plane,master   7m2s   v1.23.17
```

### 3.2 ç½‘ç»œæ’ä»¶å¤šé€‰2-[Flannel](https://github.com/flannel-io/flannel#deploying-flannel-manually)ï¼ˆä»…masterèŠ‚ç‚¹ï¼‰

#### 3.2.1 åˆ‡æ¢ç›®å½•

```bash
$ cd
$ mkdir -pv /root/k8s_soft/k8s_v1.23.17 && cd /root/k8s_soft/k8s_v1.23.17
```

#### 3.2.2 ä¸‹è½½æ–‡ä»¶

Flannelæ˜¯é…ç½®ä¸ºKubernetesè®¾è®¡çš„ç¬¬3å±‚ç½‘ç»œç»“æ„çš„ä¸€ç§ç®€å•æ˜“è¡Œçš„æ–¹æ³•ã€‚

For Kubernetes v1.17+

```bash
$ wget https://github.com/flannel-io/flannel/releases/download/v0.25.4/kube-flannel.yml
```

#### 3.2.3 æ‰§è¡Œå®‰è£…

```bash
# æŸ¥çœ‹nodes
$ kubectl get nodes
NAME   STATUS     ROLES                  AGE   VERSION
emon   NotReady   control-plane,master   16m   v1.23.17
# å®‰è£…
$ kubectl apply -f kube-flannel.yml
# æŸ¥çœ‹pods
$ kubectl get po -n kube-system -o wide
NAME                           READY   STATUS    RESTARTS   AGE     IP               NODE   NOMINATED NODE   READINESS GATES
coredns-bd6b6df9f-72cb6        1/1     Running   0          3m3s    10.244.0.3       emon   <none>           <none>
coredns-bd6b6df9f-nqfn5        1/1     Running   0          3m3s    10.244.0.2       emon   <none>           <none>
etcd-emon                      1/1     Running   0          3m17s   192.168.32.116   emon   <none>           <none>
kube-apiserver-emon            1/1     Running   0          3m18s   192.168.32.116   emon   <none>           <none>
kube-controller-manager-emon   1/1     Running   0          3m15s   192.168.32.116   emon   <none>           <none>
kube-proxy-cbb2x               1/1     Running   0          3m3s    192.168.32.116   emon   <none>           <none>
kube-scheduler-emon            1/1     Running   0          3m17s   192.168.32.116   emon   <none>           <none>
$ kubectl get nodes
NAME   STATUS   ROLES                  AGE     VERSION
emon   Ready    control-plane,master   4m29s   v1.23.17
```

### 3.3 åŠ å…¥èŠ‚ç‚¹åˆ°é›†ç¾¤ï¼ˆä»…workerèŠ‚ç‚¹ï¼‰

- åŠ å…¥é›†ç¾¤

```bash
# kubeadm initçš„æ‰§è¡Œç»“æœä¸­æœ‰å¦‚ä¸‹å‘½ä»¤ï¼Œåœ¨å„ä¸ªworkerèŠ‚ç‚¹æ‰§è¡ŒåŠ å…¥å³å¯
$ kubeadm join emon:6443 --token ldsakh.zkzpetkutui6ypmp \
        --discovery-token-ca-cert-hash sha256:7268baf811b3f1f2ca1e657fe90db99b8d3ed3f9efb8be03811b809d8efa5c5e
```

- æŸ¥çœ‹èŠ‚ç‚¹

```bash
# ç­‰èŠ‚ç‚¹åŠ å…¥æˆåŠŸï¼Œè¿‡ä¸€ä¼šæŸ¥çœ‹å¾—åˆ°
$ kubectl get nodes
NAME    STATUS   ROLES                  AGE   VERSION
emon    Ready    control-plane,master   19m   v1.23.17
emon2   Ready    <none>                 82s   v1.23.17
emon3   Ready    <none>                 45s   v1.23.17
# æ­¤æ—¶ï¼ŒworkerèŠ‚ç‚¹ä¸Šçš„å®¹å™¨å®ä¾‹å¦‚ä¸‹ï¼š
$ docker images
REPOSITORY                   TAG        IMAGE ID       CREATED        SIZE
hello-world                  latest     f1f77a0f96b7   4 months ago   5.2kB # dockeræµ‹è¯•äº§ç”Ÿçš„
registry.k8s.io/kube-proxy   v1.23.17   d3c3d806adc6   2 years ago    107MB
calico/cni                   v3.24.5    efd8ebfc4b4f   2 years ago    190MB
registry.k8s.io/pause        3.6        7d46a07936af   3 years ago    484kB
```

### 3.4 è™šæ‹ŸæœºæŒ‚èµ·å¹¶æ¢å¤åk8sç½‘ç»œé—®é¢˜ï¼ˆæ‰€æœ‰èŠ‚ç‚¹ï¼‰

é—®é¢˜æè¿°ï¼šè™šæ‹ŸæœºæŒ‚èµ·å¹¶æ¢å¤åï¼Œå„ä¸ªèŠ‚ç‚¹é€šä¿¡ä¼šå‡ºé—®é¢˜ï¼Œè®¾ç½®â€œæœªæ‰˜ç®¡â€åè§£å†³ã€‚

<span style="color:red;font-weight:bold;">è§£å†³å‰æŸ¥çœ‹ç½‘ç»œå¦‚ä¸‹æ•ˆæœï¼š</span>

```bash
# ä¸»èŠ‚ç‚¹ç½‘ç»œ
$ nmcli device status
DEVICE           TYPE      STATE         CONNECTION 
ens160           ethernet  å·²è¿æ¥        ens160     
docker0          bridge    è¿æ¥ï¼ˆå¤–éƒ¨ï¼‰  docker0    
lo               loopback  è¿æ¥ï¼ˆå¤–éƒ¨ï¼‰  lo         
tunl0            iptunnel  å·²æ–­å¼€        --         
cali4969f0a9f96  ethernet  æœªæ‰˜ç®¡        --         
cali67f6c4be37a  ethernet  æœªæ‰˜ç®¡        --         
cali6cd2b22a701  ethernet  æœªæ‰˜ç®¡        --   
# å­èŠ‚ç‚¹ç½‘ç»œ
$ nmcli device status
DEVICE   TYPE      STATE         CONNECTION 
ens160   ethernet  å·²è¿æ¥        ens160     
docker0  bridge    è¿æ¥ï¼ˆå¤–éƒ¨ï¼‰  docker0    
lo       loopback  è¿æ¥ï¼ˆå¤–éƒ¨ï¼‰  lo         
tunl0    iptunnel  å·²æ–­å¼€        --  
```

<span style="color:#32CD32;font-weight:bold;">è§£å†³åæŸ¥çœ‹ç½‘ç»œå¦‚ä¸‹æ•ˆæœï¼š</span>

```bash
# ä¸»èŠ‚ç‚¹ç½‘ç»œ
$ nmcli device status
DEVICE           TYPE      STATE         CONNECTION 
ens160           ethernet  å·²è¿æ¥        ens160     
lo               loopback  è¿æ¥ï¼ˆå¤–éƒ¨ï¼‰  lo         
docker0          bridge    æœªæ‰˜ç®¡        --         
cali4969f0a9f96  ethernet  æœªæ‰˜ç®¡        --         
cali67f6c4be37a  ethernet  æœªæ‰˜ç®¡        --         
cali6cd2b22a701  ethernet  æœªæ‰˜ç®¡        --         
tunl0            iptunnel  æœªæ‰˜ç®¡        -- 
# å­èŠ‚ç‚¹ç½‘ç»œ
$ nmcli device status
DEVICE   TYPE      STATE         CONNECTION 
ens160   ethernet  å·²è¿æ¥        ens160     
lo       loopback  è¿æ¥ï¼ˆå¤–éƒ¨ï¼‰  lo         
docker0  bridge    æœªæ‰˜ç®¡        --         
tunl0    iptunnel  æœªæ‰˜ç®¡        --  
```

- æŸ¥çœ‹è®¾å¤‡çŠ¶æ€

```bash
$ nmcli device status
```

- æ°¸ä¹…unmanaged

```bash
$ tee /etc/NetworkManager/conf.d/99-unmanaged-devices.conf << EOF
[keyfile]
unmanaged-devices=interface-name:docker*;interface-name:veth*;interface-name:br-*;interface-name:vmnet*;interface-name:vboxnet*;interface-name:cni0;interface-name:cali*;interface-name:flannel*;interface-name:tun*
EOF
```

- é‡å¯NetworkManager

```bash
$ systemctl restart NetworkManager
```

## 4 éƒ¨ç½²dashboardï¼ˆåœ¨masterèŠ‚ç‚¹æ‰§è¡Œï¼‰

[kuberneteså®˜æ–¹æä¾›çš„å¯è§†åŒ–ç•Œé¢](https://github.com/kubernetes/dashboard)

### 4.1 éƒ¨ç½²

ç‰ˆæœ¬å…¼å®¹æ€§ï¼šhttps://github.com/kubernetes/dashboard/releases 

å¯ä»¥å¾—çŸ¥dashboard v2.5.1 å…¼å®¹ kubernetes v1.23 ç‰ˆæœ¬

[dashboard v2.5.1æ–‡æ¡£](https://github.com/kubernetes/dashboard/tree/v2.5.1)

1. å®‰è£…

```bash
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.1/aio/deploy/recommended.yaml
```

> è‹¥æ— æ³•ä¸‹è½½æ–‡ä»¶ï¼Œå…ˆæ‰§è¡Œä¸‹è½½ `curl https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.1/aio/deploy/recommended.yaml -O` å†æ‰§è¡Œ `kubectl apply -f recommended.yaml`

2. è®¾ç½®è®¿é—®ç«¯å£

  - æ–¹å¼ä¸€ï¼šæ‰‹å·¥ç¼–è¾‘

  ```bash
$ kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard
  ```

  æ˜¾ç¤ºå˜æ›´çš„éƒ¨åˆ†ï¼š

  ```js
spec:
  clusterIP: 10.96.11.88
  clusterIPs:
  - 10.96.11.88
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - port: 443
    protocol: TCP
    targetPort: 8443
  selector:
    k8s-app: kubernetes-dashboard
  sessionAffinity: None
  type: ClusterIP // [!code --] [!code focus:2]
  type: NodePort // [!code ++]
status:
  loadBalancer: {}
  ```

  è®¿é—®åœ°å€ï¼š`kubectl get svc -n kubernetes-dashboard` æŸ¥çœ‹æš´éœ²çš„éšæœºç«¯å£ã€‚

  > ç¤ºä¾‹ï¼š
  >
  > ```bash
  > $ kubectl get svc -n kubernetes-dashboard
  > NAME                        TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE
  > dashboard-metrics-scraper   ClusterIP   10.96.40.11   <none>        8000/TCP        19m
  > kubernetes-dashboard        NodePort    10.96.11.88   <none>        443:30443/TCP   19m
  > ```
  >
  > å¯ä»¥çœ‹åˆ°ç«¯å£ 30443

  - æ–¹å¼äºŒï¼šå‘½ä»¤è°ƒæ•´

  ```bash
$ kubectl patch svc kubernetes-dashboard -n kubernetes-dashboard \
  -p '{"spec": {"type": "NodePort", "ports": [{"port": 443, "nodePort": 30443}]}}'
  ```

  è®¿é—®åœ°å€ï¼š
  https://<èŠ‚ç‚¹IP>:30443

---



<span style="color:red;font-weight:bold;">chromeä¸è®©è®¿é—®æ— æ•ˆè¯ä¹¦çš„httpsç½‘ç«™ï¼Œå¦‚ä½•å¤„ç†ï¼Ÿ</span>

âš ï¸ æ–¹æ³•ä¸€ï¼šç›´æ¥è¾“å…¥å¿½ç•¥å‘½ä»¤ï¼ˆæœ€ç®€å•å¿«é€Ÿï¼‰

å½“çœ‹åˆ° **`æ‚¨çš„è¿æ¥ä¸æ˜¯ç§å¯†è¿æ¥`** æˆ– **`NET::ERR_CERT_INVALID`** é”™è¯¯é¡µé¢æ—¶ï¼š

1. **å°†å…‰æ ‡ç‚¹å‡»åˆ°é”™è¯¯é¡µé¢ç©ºç™½å¤„**ï¼ˆç¡®ä¿åœ°å€æ æœªæ¿€æ´»ï¼‰ã€‚

2. **ç›´æ¥è¾“å…¥**ï¼ˆæ— éœ€ç²˜è´´ï¼‰ä»¥ä¸‹è‹±æ–‡å•è¯ï¼š

   ```
   thisisunsafe
   ```

3. é¡µé¢ä¼šè‡ªåŠ¨åˆ·æ–°å¹¶å…è®¸è®¿é—®ã€‚

> âœ… **ä¼˜ç‚¹**ï¼šæ— éœ€é‡å¯æµè§ˆå™¨æˆ–ä¿®æ”¹é…ç½®ã€‚
> âŒ **ç¼ºç‚¹**ï¼šæ¯æ¬¡è®¿é—®æ–°ç«¯å£æˆ–é‡å¯æœåŠ¡åéœ€é‡æ–°è¾“å…¥ã€‚



---



è®¿é—®é€šè¿‡åï¼Œå¯ä»¥çœ‹åˆ°å¦‚ä¸‹ç•Œé¢ï¼š

![image-20250602194303119](images/image-20250602194303119.png)

---

3. åˆ›å»ºè®¿é—®è´¦å·

[åˆ›å»ºè®¿é—®è´¦å·](https://github.com/kubernetes/dashboard/blob/v2.5.1/docs/user/access-control/creating-sample-user.md)

```bash
$ tee dashboard-adminuser.yaml <<EOF
# æˆ‘ä»¬é¦–å…ˆåœ¨å‘½åç©ºé—´ kubernetes-dashboard ä¸­åˆ›å»ºåä¸º admin-user çš„æœåŠ¡è´¦æˆ·ã€‚
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
# åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä½¿ç”¨ kops ã€ kubeadm æˆ–å…¶ä»–æµè¡Œå·¥å…·é…ç½®é›†ç¾¤åï¼Œ ClusterRole cluster-admin åœ¨é›†ç¾¤ä¸­å·²ç»å­˜åœ¨ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒï¼Œå¹¶ä»…ä¸ºæˆ‘ä»¬çš„ ServiceAccount åˆ›å»ºä¸€ä¸ª ClusterRoleBinding ã€‚å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™éœ€è¦é¦–å…ˆåˆ›å»ºæ­¤è§’è‰²ï¼Œå¹¶æ‰‹åŠ¨æˆäºˆæ‰€éœ€æƒé™ã€‚
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
EOF
```

```bash
$ kubectl apply -f dashboard-adminuser.yaml
```

4. è·å– Bearer ä»¤ç‰Œ

```bash
$ kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath="{.secrets[0].name}") -o go-template="{{.data.token | base64decode}}"
```

å®ƒåº”è¯¥ä¼šæ‰“å°å‡ºç±»ä¼¼ä»¥ä¸‹å†…å®¹ï¼š

```bash
eyJhbGciOiJSUzI1NiIsImtpZCI6InAxYVVVQWpYTFBZbzVianl5c1VKOUt1MGFtT25GNjFxTDlMOV9md09sYlkifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWp2MnRrIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI2YmFjZTU0YS0zNTliLTRhNjYtOTFiMi04MWEyODMzZDI1MDciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.T1UTl_dlX1zW09VAI3lGIYmqQI3b3Sy194KKO2HxcR7zUuf_8P8HrXivcvva3U8r7BdrKmo4aSoh-12CdjY6tui5jvg_Wmp9n212AZOhI47mQzDW4IiDRU-37Iv6yg-FRc4OnGJipYOnoAWHUxSwiVAhiCtL9PgZ9vIIde0z8EcwTWGJ896S6ugN0wBrPJHwCH3IkPRVwloPkLX9A1UQnEiSZOTHzJvvr_cAc3D95XjBT9NIvmjgHXcve74LnEE_SngJ-b-9fyqxYdzyknrGmnwNrhwle30rlr9lBSby_4x51_a7V7fK8EzgIoafNYcdIVWSE1iLtA4x-Qw-NBTvNQ
```

ç°åœ¨å¤åˆ¶è¯¥ä»¤ç‰Œï¼Œå¹¶å°†å…¶ç²˜è´´åˆ°ç™»å½•ç•Œé¢çš„ `Enter token` å­—æ®µä¸­ã€‚

![image-20250602200215542](images/image-20250602200215542.png)

ç‚¹å‡» `Sign in` æŒ‰é’®ï¼Œæå®šã€‚ä½ ç°åœ¨å·²ä»¥ç®¡ç†å‘˜èº«ä»½ç™»å½•ã€‚

5. ç•Œé¢

![image-20250602200414223](images/image-20250602200414223.png)

### 4.2 å¸è½½

- åˆ é™¤ç®¡ç†å‘˜ `ServiceAccount` å’Œ `ClusterRoleBinding` 

```bash
$ kubectl -n kubernetes-dashboard delete serviceaccount admin-user
$ kubectl -n kubernetes-dashboard delete clusterrolebinding admin-user
```

- å¸è½½dashboardç»„ä»¶

```bash
$ kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.1/aio/deploy/recommended.yaml
```

### 4.3 åŒç±»å‹è½¯ä»¶æ ¸å¿ƒå¯¹æ¯”è¡¨

| **ç‰¹æ€§**          | **Kubernetes Dashboard** | **KubeSphere**                            | **Rancher**                     |
| :---------------- | :----------------------- | :---------------------------------------- | :------------------------------ |
| **é¡¹ç›®èƒŒæ™¯**      | Kubernetes å®˜æ–¹é¡¹ç›®      | é’äº‘å¼€æº (CNCF é¡¹ç›®)                      | Rancher Labs (ç°å± SUSE)        |
| **å®šä½**          | å•é›†ç¾¤ Web UI            | **å…¨æ ˆå®¹å™¨å¹³å°**                          | **ä¼ä¸šçº§å¤šé›†ç¾¤ç®¡ç†**            |
| **å¤šé›†ç¾¤ç®¡ç†**    | âŒ ä»…å•é›†ç¾¤               | âœ… æ”¯æŒ                                    | âœ… **æ ¸å¿ƒä¼˜åŠ¿** (æ··åˆäº‘/å¤šäº‘)    |
| **éƒ¨ç½²å¤æ‚åº¦**    | â­ ç®€å• (`kubectl apply`) | â­â­â­ ä¸­ç­‰ (éœ€è§„åˆ’å­˜å‚¨/ç½‘ç»œ)                | â­â­ ä¸­ç­‰ (Helm éƒ¨ç½²)             |
| **åº”ç”¨å•†åº—**      | âŒ æ—                      | âœ… **å†…ç½®** (300+ Helm Charts)             | âœ… **å†…ç½®** (æ”¯æŒè‡ªå®šä¹‰ Catalog) |
| **DevOps æµæ°´çº¿** | âŒ æ—                      | âœ… **å®Œæ•´é›†æˆ** (Jenkins/SonarQube/GitOps) | âœ… æ”¯æŒ (éœ€é›†æˆå¤–éƒ¨å·¥å…·)         |
| **ç›‘æ§å‘Šè­¦**      | âŒ åŸºç¡€æŒ‡æ ‡               | âœ… **å¼€ç®±å³ç”¨** (Prometheus+Grafana+å‘Šè­¦)  | âœ… é›†æˆ (éœ€é¢å¤–é…ç½®)             |
| **æ—¥å¿—ç®¡ç†**      | âŒ ä»… Pod æ—¥å¿—            | âœ… **ELK/Fluentd é›†æˆ**                    | âŒ éœ€è‡ªè¡Œæ­å»º                    |
| **æœåŠ¡ç½‘æ ¼**      | âŒ æ—                      | âœ… **å†…ç½® Istio**                          | âŒ éœ€æ‰‹åŠ¨é›†æˆ                    |
| **å¤šç§Ÿæˆ·éš”ç¦»**    | â­ RBAC åŸºç¡€æ§åˆ¶          | âœ… **ä¼ä¸šçº§ç§Ÿæˆ·ä½“ç³»**                      | âœ… **ç»†ç²’åº¦ RBAC+é¡¹ç›®éš”ç¦»**      |
| **è¾¹ç¼˜è®¡ç®—æ”¯æŒ**  | âŒ æ—                      | âœ… **KubeEdge é›†æˆ**                       | âœ… **K3s è½»é‡é›†ç¾¤**              |
| **UI ä½“éªŒ**       | â­ åŠŸèƒ½å¯¼å‘ (ç®€æ´)        | â­â­â­ **ç°ä»£åŒ–æ§åˆ¶å°** (å¤šæ¨¡å—é›†æˆ)         | â­â­ åŠŸèƒ½ä¸°å¯Œ (å­¦ä¹ æ›²çº¿ç¨é™¡)      |
| **æœ€ä½³é€‚ç”¨åœºæ™¯**  | å¼€å‘è°ƒè¯•/å•é›†ç¾¤è¿ç»´      | **ä¼ä¸šçº§å…¨æ ˆå¹³å°** (DevOps+å¾®æœåŠ¡+ç›‘æ§)   | **æ··åˆäº‘/å¤§è§„æ¨¡é›†ç¾¤èˆ°é˜Ÿç®¡ç†**   |

**æ€»ç»“å»ºè®®**

- **é€‰ Kubernetes Dashboard å¦‚æœ**ï¼š
  éœ€è¦è½»é‡çº§ K8s æ“ä½œç•Œé¢ï¼Œä¸”ä»…ç®¡ç†å•ä¸ªé›†ç¾¤ã€‚
- **é€‰ KubeSphere å¦‚æœ**ï¼š
  æ„å»º **ä¸€ä½“åŒ–ä¼ä¸šå¹³å°**ï¼ˆå°¤å…¶éœ€è¦å¼€ç®±å³ç”¨çš„ DevOps/å¾®æœåŠ¡/ç›‘æ§ï¼‰ã€‚
- **é€‰ Rancher å¦‚æœ**ï¼š
  ç®¡ç† **è·¨äº‘/æ··åˆäº‘é›†ç¾¤èˆ°é˜Ÿ** æˆ–ä¸“æ³¨ **é›†ç¾¤ç”Ÿå‘½å‘¨æœŸç®¡ç†**ã€‚

> ğŸ’¡ **ç»„åˆç­–ç•¥**ï¼šå¤§å‹ä¼ä¸šå¯åŒæ—¶ä½¿ç”¨ Rancherï¼ˆå¤šé›†ç¾¤æ²»ç†ï¼‰ + KubeSphereï¼ˆé›†ç¾¤å†…åº”ç”¨å¹³å°ï¼‰ï¼Œé€šè¿‡ Rancher çº³ç®¡éƒ¨ç½²äº† KubeSphere çš„é›†ç¾¤ã€‚

## 5 å®‰è£…ingress-nginxï¼ˆåœ¨masterèŠ‚ç‚¹æ‰§è¡Œï¼‰

[ingress-nginx GitHubæŸ¥çœ‹ä¸K8Sç‰ˆæœ¬å…¼å®¹æ€§](https://github.com/kubernetes/ingress-nginx)

ingress-nginxå®˜ç½‘éƒ¨ç½²ï¼šhttps://kubernetes.github.io/ingress-nginx/deploy/

ingress-nginxå®˜ç½‘ç”¨æˆ·æŒ‡å—ï¼šhttps://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/

### 5.1 åˆ‡æ¢ç›®å½•

```bash
$ cd
$ mkdir -pv /root/k8s_soft/k8s_v1.23.17 && cd /root/k8s_soft/k8s_v1.23.17
```

### 5.2 ä¸‹è½½æ–‡ä»¶ä¸é…ç½®è°ƒæ•´

```bash
# ä¸‹è½½ https://github.com/kubernetes/ingress-nginx/blob/controller-v1.6.4/deploy/static/provider/cloud/deploy.yaml åˆ° ingress-nginx.yaml
# è‹¥ raw.githubusercontent.com æ— æ³•è®¿é—®ï¼Œå¯ä»¥é€šè¿‡ https://www.ipaddress.com æŸ¥è¯¢å…¶ipåœ°å€å¹¶é…ç½®æœ¬åœ°dns
$ curl https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.6.4/deploy/static/provider/cloud/deploy.yaml -o ingress-nginx.yaml
```

#### 5.2.1 è°ƒæ•´é•œåƒ

è‹¥ä¸è°ƒæ•´ï¼Œä¸‹è½½åå¯èƒ½æ˜¯è¿™æ ·çš„é•œåƒï¼š

```bash
registry.k8s.io/ingress-nginx/controller   <none>     81a20af4ae3c   2 years ago   282MB
registry.k8s.io/ingress-nginx/kube-webhook-certgen   <none>     7650062bc6ee   2 years ago     44.9MB
```

- è°ƒæ•´é•œåƒåç§°

```bash
$ sed -i.bak 's/image: registry.k8s.io\/ingress-nginx\/controller:v1.6.3@sha256:b92667e0afde1103b736e6a3f00dd75ae66eec4e71827d19f19f471699e909d2/image: registry.k8s.io\/ingress-nginx\/controller:v1.6.3/g;s/image: registry.k8s.io\/ingress-nginx\/kube-webhook-certgen:v20220916-gd32f8c343@sha256:39c5b2e3310dc4264d638ad28d9d1d96c4cbb2b2dcfb52368fe4e3c63f61e10f/image: registry.k8s.io\/ingress-nginx\/kube-webhook-certgen:v20220916-gd32f8c343/g' ingress-nginx.yaml
```

> è¯´æ˜ï¼šæºæ–‡ä»¶å¤‡ä»½åˆ° ingress-nginx.yaml.bak

#### 5.2.2 è°ƒæ•´Service

- è°ƒæ•´Serviceçš„typeä¸º NodePort å¹¶å›ºå®š nodePort ä¸º80å’Œ443

æ˜¾ç¤ºå˜æ›´çš„éƒ¨åˆ†ï¼š

```js
apiVersion: v1
kind: Service
metadata: 
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  externalTrafficPolicy: Local
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - appProtocol: http
    name: http
    port: 80
    protocol: TCP
    targetPort: http
    nodePort: 80 // [!code ++] [!code focus]
  - appProtocol: https
    name: https
    port: 443
    protocol: TCP
    targetPort: https
    nodePort: 443 // [!code ++] [!code focus]
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: LoadBalancer // [!code --] [!code focus:2]
  type: NodePort // [!code ++]
```

- è°ƒæ•´nodePortå…è®¸çš„ç«¯å£èŒƒå›´ï¼ˆåœ¨masterèŠ‚ç‚¹ï¼‰

ä¸Šé¢ç›´æ¥è®¾ç½®ä¸º80å’Œ443ä¼šæŠ¥é”™ï¼šnodePort: Invalid value valid ports is 30000-32767

> ä½¿ç”¨`kubectl apply`å®‰è£…æ—¶æŠ¥é”™ï¼š
>
> <span style="color:red;font-weight:bold;">The Service "ingress-nginx-controller" is invalid: spec.ports[0].nodePort: Invalid value: 80: provided port is not in the valid range. The range of valid ports is 30000-32767</span>

æ˜¯å› ä¸ºk8sçš„nodeèŠ‚ç‚¹çš„ç«¯å£é»˜è®¤è¢«é™åˆ¶åœ¨30000-32767çš„èŒƒå›´ã€‚

ä¿®æ”¹nodeèŠ‚ç‚¹çš„å…è®¸èŒƒå›´ï¼š

```bash
$ vim /etc/kubernetes/manifests/kube-apiserver.yaml 
```

åœ¨ spec.containers.command ä¸­æ‰¾åˆ°`- --service-cluster-ip-range`ï¼Œå¹¶åœ¨å…¶åå¢åŠ ä¸€è¡Œï¼š

```bash
    - --service-node-port-range=1-65535
```

- é‡å¯

```bash
$ systemctl daemon-reload && systemctl restart kubelet
```

#### 5.2.3 è°ƒæ•´Deployment

ä¿®æ”¹kindæ¨¡å¼ Deployment ==> DaemonSet

```js
apiVersion: apps/v1
kind: Deployment // [!code --] [!code focus:2]
kind: DaemonSet // [!code ++]
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx-controller
```

### 5.3 å®‰è£…ingress-nginx

- å®‰è£…æ’ä»¶ï¼ˆmasterèŠ‚ç‚¹ï¼‰

```bash
# é…ç½®èµ„æº
$ kubectl apply -f ingress-nginx.yaml
# æŸ¥çœ‹
$ kubectl get all -n ingress-nginx -o wide
NAME                                       READY   STATUS      RESTARTS   AGE   IP               NODE    NOMINATED NODE   READINESS GATES
pod/ingress-nginx-admission-create-b7mrj   0/1     Completed   0          70s   10.244.161.5     emon3   <none>           <none>
pod/ingress-nginx-admission-patch-rcgvw    0/1     Completed   0          70s   10.244.108.111   emon2   <none>           <none>
pod/ingress-nginx-controller-52dd7         1/1     Running     0          70s   10.244.161.7     emon3   <none>           <none>
pod/ingress-nginx-controller-n4mwx         1/1     Running     0          70s   10.244.108.112   emon2   <none>           <none>

NAME                                         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                 AGE   SELECTOR
service/ingress-nginx-controller             NodePort    10.96.217.34   <none>        80:80/TCP,443:443/TCP   71s   app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx
service/ingress-nginx-controller-admission   ClusterIP   10.96.60.43    <none>        443/TCP                 71s   app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx

NAME                                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE   CONTAINERS   IMAGES                                            SELECTOR
daemonset.apps/ingress-nginx-controller   2         2         2       2            2           kubernetes.io/os=linux   71s   controller   registry.k8s.io/ingress-nginx/controller:v1.6.3   app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx

NAME                                       COMPLETIONS   DURATION   AGE   CONTAINERS   IMAGES                                                                    SELECTOR
job.batch/ingress-nginx-admission-create   1/1           4s         71s   create       registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343   controller-uid=59ba2850-e57d-4ed5-9968-c37aefd14a32
job.batch/ingress-nginx-admission-patch    1/1           2s         70s   patch        registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343   controller-uid=144e3f71-16fe-4837-bf3a-e17a759e655e
```

### 5.4 æµ‹è¯•æœåŠ¡

#### 5.5.1 ingress-test.yamlé…ç½®

:::details ingress-test.yamlé…ç½®

```bash
$ tee ingress-test.yaml << EOF
#deploy
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
spec:
  selector:
    matchLabels:
      app: nginx-pod
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx-pod
    spec:
      containers:
      - name: nginx
        image: nginx:1.25.4
        ports:
        - containerPort: 80

---      
#deploy
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tomcat-deploy
spec:
  selector:
    matchLabels:
      app: tomcat-pod
  replicas: 1
  template:
    metadata:
      labels:
        app: tomcat-pod
    spec:
      containers:
      - name: tomcat
        image: tomcat:8.5-jre8-slim
        ports:
        - containerPort: 8080
        
---
#service
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx-pod
  type: ClusterIP
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80

---
#service
apiVersion: v1
kind: Service
metadata:
  name: tomcat-service
spec:
  selector:
    app: tomcat-pod
  type: ClusterIP
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080

---
#ingress
#old version: extensions/v1beta1
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-http
spec:
  ingressClassName: nginx
  rules:
  - host: nginx.fsmall.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx-service
            port:
              number: 80
  - host: tomcat.fsmall.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: tomcat-service
            port:
              number: 80
EOF
```

:::

é…ç½®èµ„æºç”Ÿæ•ˆï¼š

:::code-group

```bash [åˆ›å»º]
$ kubectl apply -f ingress-test.yaml
```

```bash [åœ¨é›†ç¾¤å†…é€šè¿‡ç›®æ ‡scvè®¿é—®]
# æŸ¥çœ‹service
$ kubectl get svc
NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes       ClusterIP   10.96.0.1       <none>        443/TCP   5d
nginx-service    ClusterIP   10.96.236.244   <none>        80/TCP    9m43s
tomcat-service   ClusterIP   10.96.8.65      <none>        80/TCP    9m43s

# å‘½ä»¤è¡Œè®¿é—®service
$ curl 10.96.236.244:80
$ curl 10.96.8.65:80
```

```bash [åœ¨é›†ç¾¤å†…é€šè¿‡ingçš„svcè®¿é—®]
# æŸ¥çœ‹ingressçš„NodePortåœ°å€
$ kubectl get svc -n ingress-nginx
NAME                                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                 AGE
ingress-nginx-controller             NodePort    10.96.217.34   <none>        80:80/TCP,443:443/TCP   15m
ingress-nginx-controller-admission   ClusterIP   10.96.60.43    <none>        443/TCP                 15m
# å‘½ä»¤è¡Œè®¿é—®service
$ curl  -H "Host: nginx.fsmall.com" 10.96.217.34:80
$ curl  -H "Host: tomcat.fsmall.com" 10.96.217.34:80
```

```bash [åœ¨é›†ç¾¤å¤–é€šè¿‡ingåŸŸåè®¿é—®]
$ kubectl get ing
NAME           CLASS   HOSTS                                ADDRESS        PORTS   AGE
ingress-http   nginx   nginx.fsmall.com,tomcat.fsmall.com   10.96.217.34   80      11m

# é…ç½®æœ¬åœ°DNSï¼šè®¿é—®emon2æˆ–emon3çš„DNS
$ vim /etc/hosts
192.168.200.117 nginx.fsmall.com
192.168.200.118 tomcat.fsmall.com
192.168.200.117 api.fsmall.com

# è®¿é—®
http://nginx.fsmall.com # çœ‹åˆ°æ­£å¸¸nginxç•Œé¢
http://tomcat.fsmall.com # çœ‹åˆ°æ­£å¸¸tomcatç•Œé¢
http://api.fsmall.com # çœ‹åˆ° nginx çš„ 404 é¡µé¢
```

```bash [åˆ é™¤]
$ kubectl delete -f ingress-test.yaml
```

:::

### 5.5 å…¶ä»–

- ingressæœåŠ¡å®‰è£…åï¼Œç¡®ä¿é›†ç¾¤ä¸­å­˜åœ¨åä¸º `nginx` çš„ IngressClassï¼š

```bash
$ kubectl get ingressclass -n ingress-nginx
```

- è‹¥`kind: Ingress`åˆ›å»ºåï¼ŒæŸ¥çœ‹`<ingress-pod-name>`æ˜¯å¦ç”Ÿæˆè§„åˆ™

```bash
# æŸ¥çœ‹ingress-pod-nameï¼Œç¡®è®¤ Nginx Ingress Controller å·²å®‰è£…ä¸” Pod æ­£å¸¸è¿è¡Œï¼š
$ kubectl get po -n ingress-nginx|grep ingress-nginx-controller
# æŸ¥çœ‹ç”Ÿæˆçš„Nginxé…ç½®
$ kubectl exec -n ingress-nginx -it <ingress-pod-name> -- cat /etc/nginx/nginx.conf
```

- è‹¥å¹¶æ²¡æœ‰ç”Ÿæˆè§„åˆ™ï¼Œæ£€æŸ¥ Ingress Controller æ—¥å¿—æ˜¯å¦æœ‰é”™è¯¯ï¼š

```bash
$ kubectl logs -n ingress-nginx <ingress-pod-name>
```



## 6 é›†ç¾¤å†’çƒŸæµ‹è¯•ï¼ˆåœ¨masterèŠ‚ç‚¹æ‰§è¡Œï¼‰

### 6.1 åˆ›å»ºnginx-ds

:::details nginx-ds.yamlé…ç½®

```bash
$ tee nginx-ds.yaml << EOF
apiVersion: v1
kind: Service
metadata:
  name: nginx-ds
  labels:
    app: nginx-ds
spec:
  type: NodePort
  selector:
    app: nginx-ds
  ports:
  - name: http
    port: 80
    targetPort: 80
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-ds
spec:
  selector:
    matchLabels:
      app: nginx-ds
  template:
    metadata:
      labels:
        app: nginx-ds
    spec:
      containers:
      - name: my-nginx
        image: nginx:1.25.4
        ports:
        - containerPort: 80
EOF
```

:::

- åˆ›å»º

```bash
$ kubectl apply -f nginx-ds.yaml
```

### 6.2 æ£€æŸ¥å„ç§ipè¿é€šæ€§

```bash
# æ£€æŸ¥å„ Node ä¸Šçš„ Pod IP è¿é€šæ€§
$ kubectl get pods -o wide

# åœ¨æ¯ä¸ªworkerèŠ‚ç‚¹ä¸Šping pod ip
# ä¸»èŠ‚ç‚¹ï¼š kubectl get pods -o wide|grep nginx-ds|awk '{print $6}'| xargs -I {} ping -c 1 "{}"
$ ping <pod-ip>

# æ£€æŸ¥serviceå¯è¾¾æ€§
$ kubectl get svc

# åœ¨æ¯ä¸ªworkerèŠ‚ç‚¹ä¸Šè®¿é—®æœåŠ¡ï¼Œè¿™é‡Œçš„<port>è¡¨ç¤ºé›†ç¾¤å†…ï¼ˆéNodePortç«¯å£ï¼‰
$ curl <service-ip>:<port>

# åœ¨æ¯ä¸ªèŠ‚ç‚¹æ£€æŸ¥node-portå¯ç”¨æ€§
$ curl <node-ip>:<port>
```

### 6.3 æ£€æŸ¥dnså¯ç”¨æ€§

:::code-group

```bash [é…ç½®]
$ cat > nginx-pod.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: docker.io/library/nginx:1.25.4
    ports:
    - containerPort: 80
EOF
```

```bash [åˆ›å»º]
$ kubectl apply -f nginx-pod.yaml
```

```bash [è¿›å…¥podæŸ¥çœ‹dns]
$ kubectl exec nginx -it -- cat /etc/resolv.conf
```

```bash [éªŒè¯è§£æ]
$ kubectl exec nginx -it -- curl nginx-ds
```

```bash [åˆ é™¤]
$ kubectl delete -f nginx-pod.yaml
```

:::

### 6.4 æ—¥å¿—åŠŸèƒ½

æµ‹è¯•ä½¿ç”¨kubectlæŸ¥çœ‹podçš„å®¹å™¨æ—¥å¿—

```bash
$ kubectl get pods
# å‘½ä»¤è¡Œè¾“å‡ºç»“æœ
NAME             READY   STATUS    RESTARTS   AGE
nginx            1/1     Running   0          54s
nginx-ds-dkfjm   1/1     Running   0          2m54s
nginx-ds-rx6mj   1/1     Running   0          2m54s

# æŸ¥çœ‹æ—¥å¿—
$ kubectl logs <pod-name>
```

### 6.5 ExecåŠŸèƒ½

æµ‹è¯•kubectlçš„execåŠŸèƒ½

```bash
# æŸ¥è¯¢æŒ‡å®šæ ‡ç­¾çš„pod
$ kubectl get pods -l app=nginx-ds
$ kubectl exec -it <nginx-pod-name> -- nginx -v
```

### 6.6 åˆ é™¤nginx-ds

```bash
$ kubectl delete -f nginx-ds.yaml
```

## 7 å­˜å‚¨æ–¹æ¡ˆ

<span style="color:red;font-weight:bold;">åœ¨dockerä¸­ï¼Œä»¥å‰æ˜¯å°†dockerå†…éƒ¨ç›®å½•æŒ‚è½½åˆ°æœºå™¨ä¸Šï¼Œä½†æ˜¯åœ¨k8sä¸­å¦‚æœå°†ç›®å½•æŒ‚è½½åˆ°æœºå™¨ä¸Šï¼Œå¦‚æœæŸä¸ªèŠ‚ç‚¹çš„å®¹å™¨æŒ‚äº†ï¼Œæ¯”å¦‚MySQLï¼Œk8sçš„è‡ªæ„ˆæœºåˆ¶ä¼šåœ¨å…¶å®ƒèŠ‚ç‚¹å†æ‹‰èµ·ä¸€ä»½ï¼Œé‚£å°±ä¼šå¯¼è‡´åŸæ¥çš„æ•°æ®ä¸¢å¤±äº†ï¼Œæ‰€ä»¥åœ¨k8sä¸­éœ€è¦åº”ç”¨åˆ°å­˜å‚¨å±‚ï¼šæ¯”å¦‚NFSã€OpenEBSï¼Œk8sä¼šå°†è¿™äº›å®¹å™¨çš„æ•°æ®å…¨éƒ¨å­˜åœ¨å­˜å‚¨å±‚ï¼Œè€Œè¿™ä¸ªå­˜å‚¨å±‚ä¼šåœ¨æ‰€æœ‰èŠ‚ç‚¹éƒ½æœ‰ä¸€ä»½ã€‚</span>

### 7.1 éƒ¨ç½²NFS

#### 7.1.1 å®‰è£… NFS æœåŠ¡ç«¯è½¯ä»¶åŒ…ï¼ˆæ‰€æœ‰èŠ‚ç‚¹ï¼‰

```bash
$ dnf install -y nfs-utils
```

#### 7.1.2 åˆ›å»ºå…±äº«æ•°æ®æ ¹ç›®å½•ï¼ˆåœ¨masterèŠ‚ç‚¹æ‰§è¡Œï¼‰

```bash
$ mkdir -pv /data/nfs/local
$ chown nobody:nobody /data/nfs/local
```

#### 7.1.3 ç¼–è¾‘æœåŠ¡é…ç½®æ–‡ä»¶ï¼ˆåœ¨masterèŠ‚ç‚¹æ‰§è¡Œï¼‰

é…ç½® NFS æœåŠ¡å™¨æ•°æ®å¯¼å‡ºç›®å½•åŠè®¿é—® NFS æœåŠ¡å™¨çš„å®¢æˆ·ç«¯æœºå™¨æƒé™ã€‚

ç¼–è¾‘é…ç½®æ–‡ä»¶ `vim /etc/exports`ï¼Œæ·»åŠ å¦‚ä¸‹å†…å®¹ï¼š

```bash
# nfsæœåŠ¡ç«¯
$ echo "/data/nfs/local 192.168.200.0/24(rw,sync,all_squash,anonuid=65534,anongid=65534,no_subtree_check)" > /etc/exports
```

- /data/nfs/localï¼šNFS å¯¼å‡ºçš„å…±äº«æ•°æ®ç›®å½•
- 192.168.200.0/24ï¼šå¯ä»¥è®¿é—® NFS å­˜å‚¨çš„å®¢æˆ·ç«¯ IP åœ°å€
- rwï¼šè¯»å†™æ“ä½œï¼Œå®¢æˆ·ç«¯æœºå™¨æ‹¥æœ‰å¯¹å·çš„è¯»å†™æƒé™ã€‚
- syncï¼šå†…å­˜æ•°æ®å®æ—¶å†™å…¥ç£ç›˜ï¼Œæ€§èƒ½ä¼šæœ‰æ‰€é™åˆ¶
- all_squashï¼šNFS å®¢æˆ·ç«¯ä¸Šçš„æ‰€æœ‰ç”¨æˆ·åœ¨ä½¿ç”¨å…±äº«ç›®å½•æ—¶éƒ½ä¼šè¢«è½¬æ¢ä¸ºä¸€ä¸ªæ™®é€šç”¨æˆ·çš„æƒé™
- anonuidï¼šè½¬æ¢åçš„ç”¨æˆ·æƒé™ IDï¼Œå¯¹åº”çš„æ“ä½œç³»ç»Ÿçš„ nobody ç”¨æˆ·
- anongidï¼šè½¬æ¢åçš„ç»„æƒé™ IDï¼Œå¯¹åº”çš„æ“ä½œç³»ç»Ÿçš„ nobody ç»„
- no_subtree_checkï¼šä¸æ£€æŸ¥å®¢æˆ·ç«¯è¯·æ±‚çš„å­ç›®å½•æ˜¯å¦åœ¨å…±äº«ç›®å½•çš„å­æ ‘èŒƒå›´å†…ï¼Œä¹Ÿå°±æ˜¯è¯´å³ä½¿è¾“å‡ºç›®å½•æ˜¯ä¸€ä¸ªå­ç›®å½•ï¼ŒNFS æœåŠ¡å™¨ä¹Ÿä¸æ£€æŸ¥å…¶çˆ¶ç›®å½•çš„æƒé™ï¼Œè¿™æ ·å¯ä»¥æé«˜æ•ˆç‡ã€‚

#### 7.1.4 å¯åŠ¨æœåŠ¡å¹¶è®¾ç½®å¼€æœºè‡ªå¯ï¼ˆåœ¨masterèŠ‚ç‚¹æ‰§è¡Œï¼‰

```bash
$ systemctl enable --now rpcbind && systemctl enable --now nfs-server
# é‡æ–°åŠ è½½ NFS å…±äº«é…ç½®ï¼ˆæ— éœ€é‡å¯æœåŠ¡ï¼‰
$ exportfs -r
# æŸ¥çœ‹å…±äº«ç›®å½•å¯¼å‡ºæƒ…å†µ
$ exportfs -v
/data/nfs/local       192.168.200.0/24(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,root_squash,all_squash)
# éªŒè¯
$ exportfs
/data/nfs/local       192.168.200.0/24
```

> **åˆ†è§£è¯´æ˜**ï¼š
>
> | å‘½ä»¤éƒ¨åˆ†   | åŠŸèƒ½                              |
> | :--------- | :-------------------------------- |
> | `exportfs` | NFS å…±äº«ç®¡ç†å·¥å…·                  |
> | `-r`       | é‡æ–°å¯¼å‡ºæ‰€æœ‰å…±äº«ï¼ˆre-export allï¼‰ |

#### 7.1.5 é…ç½®NFSä»èŠ‚ç‚¹ï¼ˆä»…workerèŠ‚ç‚¹ï¼‰

- æŸ¥çœ‹å¯ä»¥æŒ‚è½½çš„ç›®å½•

```bash
$ showmount -e 192.168.200.116
```

```bash
Export list for 192.168.200.116:
/data/nfs/local *
```

- æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æŒ‚è½½nfsæœåŠ¡å™¨ä¸Šçš„å…±äº«ç›®å½•åˆ°æœ¬æœºè·¯å¾„ /data/nfs/local

```bash
$ mkdir -p /data/nfs/local && mount -t nfs 192.168.200.116:/data/nfs/local /data/nfs/local
```

- å†™å…¥ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶ï¼ˆåœ¨NFSæœåŠ¡ç«¯ï¼‰

```bash
# æ‰§è¡Œå®Œæˆåï¼ŒæŸ¥çœ‹NFSä»èŠ‚ç‚¹åŒæ­¥ç›®å½•ï¼Œå·²ç»ç”Ÿæˆäº† test.txt æ–‡ä»¶
$ echo "hello nfs server" > /data/nfs/local/test.txt
```

#### 7.1.6 åŸç”Ÿæ–¹å¼æ•°æ®æŒ‚è½½

##### 7.1.6.1 ä¸€ä¸ªé™æ€é…ç½®æµ‹è¯•

é™æ€é…ç½®æ˜¯æŒ‡ç›´æ¥æŒ‡å®šnfsï¼›åŠ¨æ€é…ç½®æ˜¯æŒ‡é€šè¿‡StorageClassè‡ªåŠ¨åˆ›å»ºpvcï¼Œç»‘å®šåˆ°podã€‚

- é…ç½®nfs-test.yaml

```yaml
tee nfs-test.yaml << EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nfs-nginx-pv
  name: nfs-nginx-pv
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nfs-nginx-pv
  template:
    metadata:
      labels:
        app: nfs-nginx-pv
    spec:
      containers:
      - image: nginx:1.25.4
        name: nginx
        volumeMounts:
        - name: html
          mountPath: /usr/share/nginx/html
      volumes:
      - name: html
        nfs:
          server: 192.168.200.116
          path: /data/nfs/local/nginx-pv
EOF
```

- åˆ›å»º

```bash
# åœ¨ä»»ä½•NFSèŠ‚ç‚¹åˆ›å»ºç›®å½•ï¼Œè‹¥ä¸åˆ›å»ºï¼Œåœ¨Podçš„Eventsä¼šæŠ¥é”™ï¼š mounting 192.168.200.116:/data/nfs/local/nginx-pv failed, reason given by server: No such file or directory
$ mkdir -p /data/nfs/local/nginx-pv && echo 111222 > /data/nfs/local/nginx-pv/index.html
$ kubectl apply -f nfs-test.yaml
# éªŒè¯
$ curl <nfs-nginx-pv-pod-ip>:<pod-nginx-port>
```

- åˆ é™¤

```bash
$ kubectl delete -f nfs-test.yaml
```

##### 7.1.6.2 åŸç”Ÿæ–¹å¼æ•°æ®æŒ‚è½½çš„é—®é¢˜

- è¢«æŒ‚è½½çš„nfsç›®å½•ï¼Œè¦å…ˆåˆ›å»ºã€‚
- åˆ é™¤éƒ¨ç½²åï¼Œå¹¶ä¸ä¼šè‡ªåŠ¨æ¸…ç†è¢«æŒ‚è½½çš„ç›®å½•åŠå…¶ä¸‹çš„æ–‡ä»¶ã€‚
- æ¯ä¸ªè¢«æŒ‚è½½çš„ç›®å½•å¤§å°ç­‰èµ„æºå¹¶ä¸è¢«é™åˆ¶



### 7.2 å®‰è£…Kubernetes NFS Subdir External Provisioner

https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner

#### 7.2.1 è·å– NFS Subdir External Provisioner éƒ¨ç½²æ–‡ä»¶ï¼ˆåœ¨masterèŠ‚ç‚¹æ‰§è¡Œï¼‰

- ä¸‹è½½

```bash
$ wget https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner/archive/refs/tags/nfs-subdir-external-provisioner-4.0.18.tar.gz
$ tar -zxvf nfs-subdir-external-provisioner-4.0.18.tar.gz
$ cd nfs-subdir-external-provisioner-nfs-subdir-external-provisioner-4.0.18/
```

#### 7.2.2 åˆ›å»º NameSpace

**é»˜è®¤çš„ NameSpace ä¸º default**ï¼Œä¸ºäº†ä¾¿äºèµ„æºåŒºåˆ†ç®¡ç†ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°çš„å‘½åç©ºé—´ã€‚

- åˆ›å»ºNamespace

```bash
$ kubectl create ns nfs-system
```

- æ›¿æ¢èµ„æºæ¸…å•ä¸­çš„å‘½åç©ºé—´åç§°

```bash
$ sed -i'' "s/namespace:.*/namespace: nfs-system/g" ./deploy/rbac.yaml ./deploy/deployment.yaml
```

#### 7.2.3 é…ç½®å¹¶éƒ¨ç½² RBAC authorization

- åˆ›å»ºRBACèµ„æº

```bash
$ kubectl create -f deploy/rbac.yaml
```

#### 7.2.4 é…ç½®å¹¶éƒ¨ç½² NFS subdir external provisioner

è¯·ä½¿ç”¨ `vi` ç¼–è¾‘å™¨ï¼Œç¼–è¾‘æ–‡ä»¶ `deploy/deployment.yaml`ï¼Œè¯·ç”¨å®é™… NFS æœåŠ¡ç«¯é…ç½®ä¿®æ”¹ä»¥ä¸‹å†…å®¹ï¼š

1. **image:** é»˜è®¤ä½¿ç”¨ registry.k8s.io é•œåƒä»“åº“çš„é•œåƒ `nfs-subdir-external-provisioner:v4.0.2`ï¼Œç½‘ç»œå—é™æ—¶éœ€è¦æƒ³åŠæ³•ä¸‹è½½å¹¶ä¸Šä¼ åˆ°è‡ªå·±çš„é•œåƒä»“åº“

2. **192.168.200.116ï¼š** NFS æœåŠ¡å™¨çš„ä¸»æœºåæˆ–æ˜¯ IP åœ°å€

3. **/data/nfs/local:** NFS æœåŠ¡å™¨å¯¼å‡ºçš„å…±äº«æ•°æ®ç›®å½•çš„è·¯å¾„ï¼ˆexportfsï¼‰

- é…ç½®

```js
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: nfs-system
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2 // [!code focus:1]
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 192.168.200.116 // [!code focus:1]
            - name: NFS_PATH
              value: /data/nfs/local // [!code focus:1]
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.200.116 // [!code focus:1]
            path: /data/nfs/local // [!code focus:1]
```

- éƒ¨ç½²

```bash
$ kubectl apply -f deploy/deployment.yaml
```

- æŸ¥çœ‹ deploymentã€pod éƒ¨ç½²ç»“æœ

```bash
$ kubectl get deploy,po -n nfs-system
NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nfs-client-provisioner   1/1     1            1           17m

NAME                                          READY   STATUS        RESTARTS   AGE
pod/nfs-client-provisioner-5cd44d94b5-ftqr7   1/1     Running       0          3m53s
```

#### 7.2.5 éƒ¨ç½² Storage Class

**Step 1:** ç¼–è¾‘ NFS subdir external provisioner å®šä¹‰ Kubernetes Storage Class çš„é…ç½®æ–‡ä»¶  `deploy/class.yaml`ï¼Œé‡ç‚¹ä¿®æ”¹ä»¥ä¸‹å†…å®¹ï¼š

- å­˜å‚¨ç±»åç§°
- å­˜å‚¨å·åˆ é™¤åçš„é»˜è®¤ç­–ç•¥

```js
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-client // [!code --]
  name: nfs-storage // [!code ++]
  annotations: // [!code ++]
    storageclass.kubernetes.io/is-default-class: "false" # ä¸è®¾ä¸ºé»˜è®¤ // [!code ++]
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner # or choose another name, must match deployment's env PROVISIONER_NAME'
parameters:
  archiveOnDelete: "false" // [!code --]
  archiveOnDelete: "true" // [!code ++]
  pathPattern: "${.PVC.namespace}/${.PVC.name}" # è‡ªåŠ¨åˆ›å»ºç›®å½•ç»“æ„ // [!code ++]
```

é‡ç‚¹è¯´è¯´ Parameters archiveOnDelete çš„é…ç½®ã€‚

- è¯¥å€¼ä¸º false æ—¶ï¼Œå­˜å‚¨å·åˆ é™¤æ—¶ï¼Œåœ¨ NFS ä¸Šç›´æ¥åˆ é™¤å¯¹åº”çš„æ•°æ®ç›®å½•
- è¯¥å€¼ä¸º true æ—¶ï¼Œå­˜å‚¨å·åˆ é™¤æ—¶ï¼Œåœ¨ NFS ä¸Šä»¥ `archived-<volume.Name>` çš„å‘½åè§„åˆ™ï¼Œå½’æ¡£ä¿ç•™åŸæœ‰çš„æ•°æ®ç›®å½•
- **å…·ä½“å¦‚ä½•è®¾ç½®è¯·ä¸€å®šç»“åˆè‡ªå·±çš„å®é™…ç¯å¢ƒé…Œæƒ…å¤„ç†**ï¼Œæ•°æ®é‡å°çš„åœºæ™¯ä¸‹ï¼Œä¸ªäººå–œæ¬¢è®¾ç½®ä¸º trueï¼Œæ‰‹åŠ¨æˆ–è‡ªåŠ¨å®šæ—¶æ¸…ç†å½’æ¡£æ•°æ®ã€‚

**Step 2:** æ‰§è¡Œéƒ¨ç½²å‘½ä»¤ï¼Œéƒ¨ç½² Storage Classã€‚

```bash
$ kubectl apply -f deploy/class.yaml
```

- æŸ¥çœ‹ Storage Class éƒ¨ç½²ç»“æœã€‚

```bash
$ kubectl get sc
NAME          PROVISIONER                                   RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
nfs-storage   k8s-sigs.io/nfs-subdir-external-provisioner   Delete          Immediate           false                  14s
# è‹¥æ— æ³•åˆ›å»ºpvcå¯ä»¥æŸ¥çœ‹ NFS Provisioner æ—¥å¿—
$ kubectl logs -n nfs-system deploy/nfs-client-provisioner
```

### 7.3 éƒ¨ç½²OpenEBSï¼ˆæ¨èï¼‰

https://openebs.io/

é¦–å…ˆï¼Œè¯·ç¡®ä¿å®‰è£…äº†helmã€‚

- æ·»åŠ helmä»“åº“

```bash
$ helm repo add openebs https://openebs.github.io/charts
# æ›´æ–°ä»“åº“ç´¢å¼•
$ helm repo update
```

- å®‰è£…openebs

```bash
$ helm install openebs openebs/openebs \
  --namespace openebs \
  --create-namespace \
  --version 3.10.0
```

- æŸ¥çœ‹

```bash
$ helm ls -n openebs
NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION
openebs openebs         1               2025-07-12 05:26:57.179546135 +0800 CST deployed        openebs-3.10.0  3.10.0 

$ kubectl get pods -n openebs
NAME                                           READY   STATUS    RESTARTS   AGE
openebs-localpv-provisioner-668c7d88f6-rdc8r   1/1     Running   0          2m37s
openebs-ndm-operator-57fbd6b955-nhbfn          1/1     Running   0          24m
openebs-ndm-xmxh9                              1/1     Running   0          24m
openebs-ndm-zbbnl                              1/1     Running   0          24m

$ kubectl get sc
NAME               PROVISIONER                                   RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
nfs-storage        k8s-sigs.io/nfs-subdir-external-provisioner   Delete          Immediate              false                  111m
openebs-device     openebs.io/local                              Delete          WaitForFirstConsumer   false                  24m
openebs-hostpath   openebs.io/local                              Delete          WaitForFirstConsumer   false                  24m
```

- è®¾ç½®é»˜è®¤å­˜å‚¨ç±»

**å¿…é¡»è¦è®¾ç½®é»˜è®¤å­˜å‚¨ç±»ï¼Œä¸ç„¶å®‰è£…kubesphereçš„æ—¶å€™ï¼Œä¼šæŠ¥é”™ï¼Œæ‰¾ä¸åˆ°é»˜è®¤å­˜å‚¨ç±»**

```bash
$ kubectl patch storageclass openebs-hostpath -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

$ kubectl get sc
NAME                         PROVISIONER                                   RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
nfs-storage                  k8s-sigs.io/nfs-subdir-external-provisioner   Delete          Immediate              false                  117m
openebs-device               openebs.io/local                              Delete          WaitForFirstConsumer   false                  30m
openebs-hostpath (default)   openebs.io/local                              Delete          WaitForFirstConsumer   false                  30m
```



| **åœºæ™¯**              | **æ¨èå­˜å‚¨ç±»**     |
| :-------------------- | :----------------- |
| å¼€å‘/æµ‹è¯•ç¯å¢ƒ         | `openebs-hostpath` |
| ç”Ÿäº§ç¯å¢ƒ - é«˜æ€§èƒ½éœ€æ±‚ | `openebs-device`   |

### 7.3 å­˜å‚¨æ–¹æ¡ˆå¯¹æ¯”ä¸é€‰æ‹©

#### 7.3.1 OpenEBSï¼šäº‘åŸç”Ÿå­˜å‚¨è§£å†³æ–¹æ¡ˆ

```mermaid
graph LR
    A[Kubernetes] --> B(OpenEBS Control Plane)
    B --> C[cStor Engine]
    B --> D[Jiva Engine]
    B --> E[LocalPV Engine]
    C --> F[Replicated Block Storage]
    D --> G[Lightweight Block Storage]
    E --> H[Direct Local Storage]
```

**æ ¸å¿ƒç‰¹æ€§**ï¼š

- **å®¹å™¨åŸç”Ÿæ¶æ„**ï¼šæ¯ä¸ªå·éƒ½æœ‰è‡ªå·±çš„å­˜å‚¨æ§åˆ¶å™¨ï¼ˆContainer Attached Storageï¼‰
- **å¤šå­˜å‚¨å¼•æ“**ï¼š
  - cStorï¼šä¼ä¸šçº§å­˜å‚¨ï¼ˆå¿«ç…§ã€å…‹éš†ã€ç²¾ç®€é…ç½®ï¼‰
  - Jivaï¼šè½»é‡çº§åŸºäºå®¹å™¨çš„å­˜å‚¨
  - LocalPVï¼šç›´æ¥ä½¿ç”¨æœ¬åœ°ç£ç›˜
- **å®Œå…¨å¼€æº**ï¼šCNCF æ²™ç®±é¡¹ç›®
- **Kubernetes åŸç”Ÿé›†æˆ**ï¼šé€šè¿‡ StorageClass åŠ¨æ€é…ç½®å­˜å‚¨

#### 7.3.2 NFSï¼šä¼ ç»Ÿç½‘ç»œæ–‡ä»¶ç³»ç»Ÿ

```mermaid
graph LR
    A[Kubernetes Pods] --> B[NFS Client]
    B --> C[NFS Server]
    C --> D[ç‰©ç†å­˜å‚¨]
```

**æ ¸å¿ƒç‰¹æ€§**ï¼š

- **æ ‡å‡†åŒ–åè®®**ï¼šRFC å®šä¹‰çš„æ ‡å‡†ç½‘ç»œæ–‡ä»¶ç³»ç»Ÿ
- **å…±äº«å­˜å‚¨**ï¼šå¤šä¸ªå®¢æˆ·ç«¯åŒæ—¶è®¿é—®ç›¸åŒå­˜å‚¨
- **ç®€å•æ˜“ç”¨**ï¼šæˆç†Ÿçš„ç”Ÿæ€ç³»ç»Ÿå’Œå·¥å…·é“¾
- **è·¨å¹³å°å…¼å®¹**ï¼šæ”¯æŒæ‰€æœ‰ä¸»æµæ“ä½œç³»ç»Ÿ

**å…³é”®ç»´åº¦å¯¹æ¯”ï¼š**

| **ç‰¹æ€§**       | **OpenEBS**               | **NFS**                               |
| :------------- | :------------------------ | :------------------------------------ |
| **æ¶æ„æ¨¡å‹**   | å®¹å™¨é™„åŠ å­˜å‚¨ (CAS)        | å®¢æˆ·ç«¯-æœåŠ¡å™¨æ¨¡å‹                     |
| **å­˜å‚¨ç±»å‹**   | å—å­˜å‚¨ (iSCSI)            | æ–‡ä»¶å­˜å‚¨ (NAS)                        |
| **æ•°æ®ä¸€è‡´æ€§** | å¼ºä¸€è‡´æ€§ (cStor)          | æœ€ç»ˆä¸€è‡´æ€§ (NFSv3) / å¼ºä¸€è‡´æ€§ (NFSv4) |
| **æ€§èƒ½**       | é«˜ (æœ¬åœ°ç£ç›˜è®¿é—®)         | ä¸­ (ç½‘ç»œä¾èµ–)                         |
| **éƒ¨ç½²å¤æ‚åº¦** | ä¸­ (Kubernetes Operator)  | ä½ (ç‹¬ç«‹æœåŠ¡å™¨)                       |
| **æ‰©å±•æ€§**     | æ°´å¹³æ‰©å±• (æ·»åŠ å­˜å‚¨èŠ‚ç‚¹)   | å‚ç›´æ‰©å±• (å‡çº§æœåŠ¡å™¨)                 |
| **é«˜å¯ç”¨æ€§**   | å†…ç½® (cStor å‰¯æœ¬)         | éœ€é¢å¤–é…ç½® (DRBD+Keepalived)          |
| **å¿«ç…§/å…‹éš†**  | åŸç”Ÿæ”¯æŒ                  | éœ€å­˜å‚¨è®¾å¤‡æ”¯æŒ                        |
| **é€‚ç”¨åœºæ™¯**   | æœ‰çŠ¶æ€åº”ç”¨ã€æ•°æ®åº“ã€AI/ML | å…±äº«å­˜å‚¨ã€å†…å®¹ç®¡ç†                    |

#### 7.33 æ··åˆæ¨¡å¼

**ç»“è®ºä¸æ¨èï¼š**

| **åœºæ™¯**                  | **æ¨èæ–¹æ¡ˆ**       | **ç†ç”±**                        |
| :------------------------ | :----------------- | :------------------------------ |
| **é«˜æ€§èƒ½æ•°æ®åº“**          | OpenEBS cStor      | ä½å»¶è¿Ÿã€é«˜ IOPSã€æ•°æ®ä¸€è‡´æ€§ä¿è¯ |
| **å…±äº«æ–‡ä»¶å­˜å‚¨**          | NFS                | æˆç†Ÿåè®®ã€å¤šå®¢æˆ·ç«¯è®¿é—®          |
| **è¾¹ç¼˜è®¡ç®—/èµ„æºå—é™ç¯å¢ƒ** | OpenEBS LocalPV    | è½»é‡çº§ã€é›¶ç®¡ç†å¼€é”€              |
| **ä¼ä¸šå†…å®¹ç®¡ç†ç³»ç»Ÿ**      | NFS                | ç®€å•å…±äº«å­˜å‚¨éœ€æ±‚                |
| **å¤šé›†ç¾¤/æ··åˆäº‘éƒ¨ç½²**     | OpenEBS + NFS æ··åˆ | çµæ´»åº”å¯¹ä¸åŒéœ€æ±‚                |
| **å¼€å‘æµ‹è¯•ç¯å¢ƒ**          | NFS                | å¿«é€Ÿéƒ¨ç½²ã€ä½æˆæœ¬                |

**æœ€ç»ˆå»ºè®®**ï¼š

1. **æ–°éƒ¨ç½²æœ‰çŠ¶æ€åº”ç”¨**ï¼šé¦–é€‰ OpenEBS cStor
2. **å…±äº«å­˜å‚¨éœ€æ±‚**ï¼šä½¿ç”¨ NFS æˆ–è€ƒè™‘ CephFS
3. **æ€§èƒ½æ•æ„Ÿå‹å·¥ä½œè´Ÿè½½**ï¼šOpenEBS LocalPV
4. **æ··åˆç¯å¢ƒ**ï¼šç»„åˆä½¿ç”¨ OpenEBS å’Œ NFS
5. **é•¿æœŸæ¼”è¿›**ï¼šé€æ­¥å°†å…³é”®åº”ç”¨è¿ç§»åˆ° OpenEBS

> åœ¨ Kubernetes ç”Ÿæ€ä¸­ï¼ŒOpenEBS ä»£è¡¨äº†å­˜å‚¨çš„æœªæ¥æ–¹å‘ï¼Œè€Œ NFS åˆ™æ˜¯æˆç†Ÿçš„ä¼ ç»Ÿè§£å†³æ–¹æ¡ˆã€‚æ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©æˆ–ç»„åˆä¸¤è€…ï¼Œå¯ä»¥å®ç°æœ€ä¼˜çš„å­˜å‚¨æ¶æ„ã€‚

## 8 Harboré•œåƒç§æœï¼ˆåœ¨emonä¸»æœºrootç”¨æˆ·å®‰è£…ï¼‰

### 8.1 å®‰è£…docker-compose

1ï¼šä¸‹è½½

```bash
$ curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
```

2ï¼šæ·»åŠ å¯æ‰§è¡Œæƒé™

```bash
$ chmod +x /usr/local/bin/docker-compose
# åˆ›å»ºè½¯è¿ï¼Œé¿å…å®‰è£…Harboræ—¶æŠ¥é”™ï¼š? Need to install docker-compose(1.18.0+) by yourself first and run this script again.
$ ln -snf /usr/local/bin/docker-compose /usr/bin/docker-compose
```

33ï¼šæµ‹è¯•

```bash
$ docker-compose --version
# å‘½ä»¤è¡Œè¾“å‡ºç»“æœ
docker-compose version 1.29.2, build 5becea4c
```

### 8.2 å®‰è£…Harboré•œåƒç§æœ

Harboré•œåƒç§æœï¼ˆåœ¨emonä¸»æœºrootç”¨æˆ·å®‰è£…ï¼‰

0. åˆ‡æ¢ç›®å½•

```bash
$ cd
$ mkdir -pv /root/k8s_soft/k8s_v1.20.15 && cd /root/k8s_soft/k8s_v1.20.15
```

1. ä¸‹è½½åœ°å€

https://github.com/goharbor/harbor/releases

```bash
$ wget https://github.com/goharbor/harbor/releases/download/v2.2.4/harbor-offline-installer-v2.2.4.tgz
```

2. åˆ›å»ºè§£å‹ç›®å½•

```bash
# åˆ›å»ºHarborè§£å‹ç›®å½•
$ mkdir /usr/local/Harbor
# åˆ›å»ºHarborçš„volumeç›®å½•
$ mkdir -p /usr/local/dockerv/harbor_home
```

3. è§£å‹

```bash
# æ¨èv2.2.4ç‰ˆæœ¬ï¼Œæ›´é«˜ç‰ˆæœ¬æ¯”å¦‚2.3å’Œ2.4æœ‰docker-compose down -v ==> down-compose up -dæ—¶postgresqlæœåŠ¡å¯åŠ¨ä¸äº†çš„bugï¼Œæ•°æ®åº“é‡å¯å¤±è´¥ï¼
$ tar -zxvf harbor-offline-installer-v2.2.4.tgz -C /usr/local/Harbor/
$ ls /usr/local/Harbor/harbor
common.sh  harbor.v2.2.4.tar.gz  harbor.yml.tmpl  install.sh  LICENSE  prepare
```

4. åˆ›å»ºè‡ªç­¾åè¯ä¹¦ã€å‚è€ƒå®ç°ï¼Œå»ºè®®èµ°æ­£è§„æ¸ é“çš„CAè¯ä¹¦ã€‘ã€ç¼ºå°‘è¯ä¹¦æ— æ³•æµè§ˆå™¨ç™»å½•ã€‘

- åˆ›å»ºè¯ä¹¦å­˜æ”¾ç›®å½•

```bash
# åˆ‡æ¢ç›®å½•
$ mkdir /usr/local/Harbor/cert && cd /usr/local/Harbor/cert
```

- åˆ›å»ºCAæ ¹è¯ä¹¦

```bash
# å…¶ä¸­Cæ˜¯Countryï¼ŒSTæ˜¯Stateï¼ŒLæ˜¯localï¼ŒOæ˜¯Origanizationï¼ŒOUæ˜¯Organization Unitï¼ŒCNæ˜¯common name(eg, your name or your server's hostname)
$ openssl req -newkey rsa:4096 -nodes -sha256 -keyout ca.key -x509 -days 3650 -out ca.crt \
-subj "/C=CN/ST=ZheJiang/L=HangZhou/O=HangZhou emon Technologies,Inc./OU=IT emon/CN=emon"
# æŸ¥çœ‹ç»“æœ
$ ls
ca.crt  ca.key
```

- ç”Ÿæˆä¸€ä¸ªè¯ä¹¦ç­¾åï¼Œè®¾ç½®è®¿é—®åŸŸåä¸º emon

```bash
$ openssl req -newkey rsa:4096 -nodes -sha256 -keyout emon.key -out emon.csr \
-subj "/C=CN/ST=ZheJiang/L=HangZhou/O=HangZhou emon Technologies,Inc./OU=IT emon/CN=emon"
# æŸ¥çœ‹ç»“æœ
$ ls
ca.crt  ca.key  emon.csr  emon.key
```

- ç”Ÿæˆä¸»æœºçš„è¯ä¹¦

```bash
$ openssl x509 -req -days 3650 -in emon.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out emon.crt
# æŸ¥çœ‹ç»“æœ
$ ls
ca.crt  ca.key  ca.srl  emon.crt  emon.csr  emon.key
```

5. ç¼–è¾‘é…ç½®

```bash
$ cp /usr/local/Harbor/harbor/harbor.yml.tmpl /usr/local/Harbor/harbor/harbor.yml
$ vim /usr/local/Harbor/harbor/harbor.yml
```

```yaml
# ä¿®æ”¹
# hostname: reg.mydomain.com
hostname: 192.168.32.116
# ä¿®æ”¹
  # port: 80
  port: 5080
# ä¿®æ”¹
https:
  # https port for harbor, default is 443
  port: 5443
  # The path of cert and key files for nginx
  # certificate: /your/certificate/path
  # private_key: /your/private/key/path
  # ä¿®æ”¹ï¼šæ³¨æ„ï¼Œè¿™é‡Œä¸èƒ½ä½¿ç”¨è½¯è¿æ¥ç›®å½• /usr/loca/harboræ›¿æ¢/usr/local/Harbor/harbor-2.2.4
  # å¦åˆ™ä¼šå‘ç”Ÿè¯ä¹¦æ‰¾ä¸åˆ°é”™è¯¯ï¼šFileNotFoundError: [Errno 2] No such file or directory: 
  certificate: /usr/local/Harbor/cert/emon.crt
  private_key: /usr/local/Harbor/cert/emon.key
# ä¿®æ”¹
# data_volume: /data
data_volume: /usr/local/dockerv/harbor_home
```

6. å®‰è£…

```bash
# å®‰è£…æ—¶ï¼Œç¡®ä¿ /usr/bin/docker-compose å­˜åœ¨ï¼Œå¦åˆ™ä¼šæŠ¥é”™ï¼š? Need to install docker-compose(1.18.0+) by yourself first and run this script again.
$ /usr/local/Harbor/harbor/install.sh --with-chartmuseum --with-trivy
# åˆ‡æ¢ç›®å½•
$  cd /usr/local/Harbor/harbor/
# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
$ docker-compose ps
# å‘½ä»¤è¡Œè¾“å‡ºç»“æœ
      Name                     Command                  State                           Ports                     
------------------------------------------------------------------------------------------------------------------
chartmuseum         ./docker-entrypoint.sh           Up (healthy)                                                 
harbor-core         /harbor/entrypoint.sh            Up (healthy)                                                 
harbor-db           /docker-entrypoint.sh 96 13      Up (healthy)                                                 
harbor-jobservice   /harbor/entrypoint.sh            Up (healthy)                                                 
harbor-log          /bin/sh -c /usr/local/bin/ ...   Up (healthy)   127.0.0.1:1514->10514/tcp                     
harbor-portal       nginx -g daemon off;             Up (healthy)                                                 
nginx               nginx -g daemon off;             Up (healthy)   0.0.0.0:5080->8080/tcp, 0.0.0.0:5443->8443/tcp
redis               redis-server /etc/redis.conf     Up (healthy)                                                 
registry            /home/harbor/entrypoint.sh       Up (healthy)                                                 
registryctl         /home/harbor/start.sh            Up (healthy)                                                 
trivy-adapter       /home/scanner/entrypoint.sh      Up (healthy)
```

8. ç™»å½•

è®¿é—®ï¼šhttp://192.168.32.116:5080 ï¼ˆä¼šè¢«è·³è½¬åˆ°http://192.168.32.116:5443ï¼‰

ç”¨æˆ·åå¯†ç ï¼š admin/Harbor12345

harboræ•°æ®åº“å¯†ç ï¼š root123

ç™»å½•ååˆ›å»ºäº†ç”¨æˆ·ï¼šemon/Emon@123

ç™»å½•ååˆ›å»ºäº†å‘½åç©ºé—´ï¼šdevops-learning å¹¶å°†emonç”¨æˆ·ç”¨äºè¯¥å‘½åç©ºé—´

9. ä¿®æ”¹é…ç½®é‡å¯

```bash
$ cd /usr/local/Harbor/harbor/
$ docker-compose down -v
# å¦‚æœç¢°åˆ° postgresql æœåŠ¡ä¸æ˜¯UPçŠ¶æ€ï¼Œå¯¼è‡´ç™»å½•æç¤ºï¼šæ ¸å¿ƒæœåŠ¡ä¸å¯ç”¨ã€‚ è¯·æ‰§è¡Œä¸‹é¢å‘½ä»¤ï¼ˆæ ¹æ®data_volumeé…ç½®è°ƒæ•´è·¯å¾„ï¼‰ï¼Œè¿™ä¸ªæ˜¯è¯¥ç‰ˆæœ¬çš„bugã€‚ç›®å‰ï¼Œv2.2.4ç‰ˆæœ¬å¯ä»¥æ­£ç¡®é‡å¯ï¼Œæ— éœ€åˆ é™¤pg13
# [emon@emon harbor]$ sudo rm -rf /usr/local/dockerv/harbor_home/database/pg13
$ docker-compose up -d
```

10. ç§æœå®‰å…¨æ§åˆ¶

- å¯¹æ–‡ä»¶ `/etc/docker/daemon.json` è¿½åŠ  `insecure-registries`å†…å®¹ï¼š

```bash
$ vim /etc/docker/daemon.json
```

```bash
{
  "registry-mirrors": ["https://pyk8pf3k.mirror.aliyuncs.com","https://dockerproxy.com","https://mirror.baidubce.com","https://docker.nju.edu.cn","https://docker.mirrors.sjtug.sjtu.edu.cn","https://docker.mirrors.ustc.edu.cn"],
  "graph": "/usr/local/lib/docker",
  "exec-opts": ["native.cgroupdriver=cgroupfs"],
  "insecure-registries": ["192.168.32.116:5080"]
}
```

- å¯¹æ–‡ä»¶ `/lib/systemd/system/docker.service` è¿½åŠ `EnvironmentFile`ï¼šã€å¯çœç•¥ã€‘

```bash
$ vim /lib/systemd/system/docker.service 
```

```bash
# åœ¨ExecStartåé¢ä¸€è¡Œè¿½åŠ ï¼šç»éªŒè¯daemon.jsoné…ç½®äº†insecure-registrieså³å¯ï¼Œæ— éœ€è¿™é‡Œå†é…ç½®
EnvironmentFile=-/etc/docker/daemon.json
```

é‡å¯DockeræœåŠ¡ï¼š

```bash
$ systemctl daemon-reload
$ systemctl restart docker
```

10. æ¨é€é•œåƒ

ç™»å½•harboråï¼Œå…ˆåˆ›å»ºdevops-learningé¡¹ç›®ï¼Œå¹¶åˆ›å»ºemonç”¨æˆ·ã€‚

```bash
# ä¸‹è½½
$ docker pull openjdk:8-jre
# æ‰“æ ‡ç­¾
$ docker tag openjdk:8-jre 192.168.32.116:5080/devops-learning/openjdk:8-jre
# ç™»å½•
$ docker login -u emon -p Emon@123 192.168.32.116:5080
# ä¸Šä¼ é•œåƒ
$ docker push 192.168.32.116:5080/devops-learning/openjdk:8-jre
# é€€å‡ºç™»å½•
$ docker logout 192.168.32.116:5080

æœºå™¨äººè´¦æˆ·ï¼š
tokenï¼š  
XsttKM4zpuFWcchUmEhJErmiRRRfBu0A
```

## 9 æ–°ä»¤ç‰Œä¸è¯ä¹¦

### 9.1 kubeadmå¦‚ä½•åŠ å…¥èŠ‚ç‚¹ï¼ˆåœ¨masterèŠ‚ç‚¹æ‰§è¡Œï¼‰

- é‡æ–°ç”Ÿæˆæ–°çš„token

```bash
$ kubeadm token create --print-join-command
```

```bash
kubeadm join emon:6443 --token yslydb.mkmtnbjpfkuaa85n --discovery-token-ca-cert-hash sha256:7268baf811b3f1f2ca1e657fe90db99b8d3ed3f9efb8be03811b809d8efa5c5e 
```

> - æŸ¥çœ‹æ‰€æœ‰tokenåˆ—è¡¨
>
> ```bash
> $ kubeadm token list
> ```
>
> - è·å–caè¯ä¹¦sha256ç¼–ç hashå€¼
>
> ```bash
> $ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
> ```
>



- ç”Ÿæˆä¸€ä¸ªæ°¸ä¸è¿‡æœŸçš„token

```bash
$ token=$(kubeadm token generate)
$ kubeadm token create $token --print-join-command --ttl=0
```

è¯´æ˜ï¼š`--ttl=0`,è¡¨ç¤ºæ°¸ä¸å¤±æ•ˆ



- åˆ é™¤token

```bash
$ kubeadm delete [token-value] ...
```

> ç¤ºä¾‹ï¼š`kubeadm token delete yslydb.mkmtnbjpfkuaa85n nbdvuh.whaq4d2xm5vr6cih`

### 9.2 æŸ¥çœ‹kubeadmæ­å»ºé›†ç¾¤çš„è¯ä¹¦è¿‡æœŸæ—¶é—´ï¼ˆæ‰€æœ‰èŠ‚ç‚¹çš†å¯ï¼‰

```bash
$ cd /etc/kubernetes/pki/ && for i in $(ls *.crt); do echo "===== $i ====="; openssl x509 -in $i -text -noout | grep -A 3 'Validity' ; done
```

- ä½¿ç”¨ **kubeadm** æŸ¥çœ‹

```bash
$ kubeadm certs check-expiration
```

- ä½¿ç”¨ **kk** æŸ¥çœ‹

```bash
$ ./kk certs check-expiration -f ksp-k8s-v1306.yaml
```



# è¡¥å…… æ¼”ç»ƒä¸ç†è§£

## 1 å¸¸è§„å‘½ä»¤éƒ¨ç½²ä¸€ä¸ªtomcat

```bash
# éƒ¨ç½²ä¸€ä¸ªtomcat
$ kubectl create deployment tomcat8 --image=tomcat:8.5-jre8-slim
# æš´éœ²nginxè®¿é—®ï¼ŒPodçš„80æ˜ å°„å®¹å™¨çš„8080ï¼›serviceä¼šä»£ç†Podçš„80.
$ kubectl expose deployment tomcat8 --port=80 --target-port=8080 --type=NodePort
# æŸ¥è¯¢NodePortç«¯å£
$ kubectl get svc|grep tomcat8
tomcat8      NodePort    10.96.82.16   <none>        80:3736/TCP   86s
# è®¿é—® http://192.168.32.116:3736
# æŸ¥çœ‹æ‰€æœ‰
$ kubectl get all

# æ‰©å®¹ï¼šæ‰©å®¹äº†å¤šä»½ï¼Œæ‰€ä»¥æ— è®ºè®¿é—®å“ªä¸ªnodeçš„æŒ‡å®šç«¯å£ï¼Œéƒ½å¯ä»¥è®¿é—®åˆ°tomcat6
$ kubectl scale --replicas=3 deployment tomcat8
# åˆ é™¤
$ kubectl delete deployment.apps/tomcat8 service/tomcat8
```

- æŸ¥çœ‹éƒ¨ç½²ä¸€ä¸ªtomcatå¯¹åº”çš„yamlä¿¡æ¯

```bash
$ kubectl create deployment tomcat8 --image=tomcat:8.5-jre8-slim --dry-run=client -o yaml > tomcat8-deploy.yml
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: tomcat8
  name: tomcat8
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tomcat8
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: tomcat8
    spec:
      containers:
      - image: tomcat:8.5-jre8-slim
        name: tomcat
        resources: {}
status: {}
```

```bash
$ kubectl apply -f tomcat8.yml
```

- æŸ¥çœ‹æš´éœ²nginxè®¿é—®å¯¹åº”çš„yamlä¿¡æ¯

```bash
$ kubectl expose deployment tomcat8 --port=80 --target-port=8080 --type=NodePort --dry-run=client -o yaml > tomcat8-svc.yml
```

```yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: tomcat8
  name: tomcat8
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: tomcat8
  type: NodePort
status:
  loadBalancer: {}
```

- æŸ¥çœ‹æš´éœ²podå¯¹åº”çš„yamlä¿¡æ¯

```bash
$ kubectl get pods tomcat8-5796df556f-rzdf6 -o yaml > pod.yaml
```

## 2 é€šè¿‡yamléƒ¨ç½²ä¸€ä¸ªtomcat

- å‡†å¤‡ä¸€ä¸ªéƒ¨ç½²

```bash
$ vim tomcat8.yml
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: tomcat8
  name: tomcat8-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: tomcat8-pod
  template:
    metadata:
      labels:
        app: tomcat8-pod
    spec:
      containers:
      - image: tomcat:8.5-jre8-slim
        name: tomcat
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: tomcat8
  name: tomcat8-service
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: tomcat8-pod
  type: NodePort

---
#ingress
#old version: extensions/v1beta1
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-http
spec:
  ingressClassName: nginx
  rules:
  - host: tomcat.fsmall.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: tomcat8-service
            port:
              number: 80
```

```bash
$ kubectl apply -f tomcat8.yaml
```



